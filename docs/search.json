[
  {
    "objectID": "workshop.html",
    "href": "workshop.html",
    "title": "Workshop",
    "section": "",
    "text": "Workshop on Cloud Computing organized by IIT Kharagpur,2019"
  },
  {
    "objectID": "taylor.html",
    "href": "taylor.html",
    "title": "Taylor Series",
    "section": "",
    "text": "\\[\\begin{equation}\nf(x) = \\sum_{n=0}^{\\infty}\\frac{f^{(n)}(a)}{n!}(x-a)^{n}\n\\end{equation}\\]\nLet \\(f(x)\\) be a function that is \\(n+1\\) times differentiable on an interval \\(I\\) containing \\(a\\) and let \\(P_n(x)\\) be the \\(n\\)th degree Taylor polynomial for \\(f(x)\\) about \\(a\\). Then, there exists a number \\(c\\) between \\(a\\) and \\(x\\) such that: \\[\\begin{equation}\nf(x)=P_n(x)+R_n(x),\n\\end{equation}\\] where the remainder \\(R_n(x)\\) is given by: \\[\\begin{equation}\nR_n(x)=\\frac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1}.\n\\end{equation}\\]\n\nDefine the function to be approximated\n\n\nCode\nimport torch\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\n# Define the sine function to be approximated\ndef f(x):\n    return torch.sin(x)\n\nx = torch.linspace(-3.14, 3.14, 100)\ny = f(x)\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Sine Function')\nplt.show()\n\n\n\n\n\n\n\nFirst order Taylor approximation for f(x) at x = 0\n\n\nCode\nx = torch.tensor([0.], requires_grad=True)\ny = f(x)\napprox = y + torch.autograd.grad(y, x, create_graph=True)[0] * x\nx_vals = torch.linspace(-np.pi, np.pi, 100)\ny_vals = f(x_vals)\napprox_vals = (approx.detach() + torch.autograd.grad(approx, x, create_graph=True)[0] * x_vals).detach()\nplt.plot(x_vals.numpy(), y_vals.numpy(), label='sin(x)')\nplt.plot(x_vals.numpy(), approx_vals.numpy(), label='approx')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nFind the nth order Taylor approximation for f(x) at x = 0\n\n\nCode\ndef fact(n):\n    return math.factorial(n)\n\ndef nth_deriv(f, a, n):\n    if isinstance(a, (float, int)):\n        a = torch.tensor([a], dtype=torch.float, requires_grad=True)\n    else:\n        a = a.clone().detach().requires_grad_(True)\n    \n    y = f(a)\n    for i in range(n):\n        y = torch.autograd.grad(y, a, create_graph=True)[0]\n    return y\n\n\n\n# nth degree Taylor polynomial of f(x) around x=a\ndef taylor(f, x, n):\n    result = torch.zeros_like(x)\n    for i in range(n+1):\n        result += nth_deriv(f, 0, i) / torch.tensor(math.factorial(i), dtype=torch.float32) * (x**i)\n    return result\nx_vals = torch.linspace(-math.pi, math.pi, 200)\nplt.plot(x_vals.numpy(), f(x_vals).numpy(), label='f(x)', lw=5)\nplt.plot(x_vals.numpy(), taylor(f, x_vals, 1).detach().numpy(), label='Taylor approximation, n=1')\nplt.plot(x_vals.numpy(), taylor(f, x_vals, 3).detach().numpy(), label='Taylor approximation, n=3')\nplt.plot(x_vals.numpy(), taylor(f, x_vals, 5).detach().numpy(), label='Taylor approximation, n=5')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nPlot of the function g(x) = x^2 and its Taylor approximations up to degree 1 and degree 2 centered at x=0.\n\n\nCode\nx_vals = torch.linspace(-4, 4, 100)\n\ndef g(x):\n    return x**2\n\ndef taylor(f, x, n):\n    x = x.unsqueeze(-1)\n    y = f(x)\n    for i in range(1, n+1):\n        y += (x - x[0])**i / torch.tensor([math.factorial(i)]).float() * f(x[0] + 0.0)\n    return y\n\n\nplt.plot(x_vals.numpy(), g(x_vals).numpy(), label='g(x)', lw=5)\nplt.plot(x_vals.numpy(), taylor(g, x_vals, 1).detach().numpy(), label='Taylor approximation, n=1')\nplt.plot(x_vals.numpy(), taylor(g, x_vals, 3).detach().numpy(), label='Taylor approximation, n=3')\nplt.plot(x_vals.numpy(), taylor(g, x_vals, 5).detach().numpy(), label='Taylor approximation, n=5')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "Conference.html",
    "href": "Conference.html",
    "title": "Publications",
    "section": "",
    "text": "Scalable Methods for Brick Kiln Detection and Compliance Monitoring from Satellite Imagery: A Deployment Case Study in India - Arxiv.\n[code]  [App] \nTowards Scalable Identification of Brick Kilns from Satellite Imagery with Active Learning - NeurIPS 2023 Workshop.\n[code]\nRecall-Driven Precision Refinement: Unveiling Accurate Fall Detection Using LSTM - 6TH IFIP IOT CONFERENCE, TEXAS, USA."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Certification",
    "section": "",
    "text": "Deep Learning from GUVI Geek Network & IITM - 2021.\nSix weeks of Summer Training on ML from IIT Kanpur,2019.\nOnline Python Training from IIT Kanpur,2019."
  },
  {
    "objectID": "posts/MLLosses.html",
    "href": "posts/MLLosses.html",
    "title": "Loss and Optimization",
    "section": "",
    "text": "The Loss Function is a method of evaluating how well a machine learning algorithm models a featured data set. If our loss function value is low, our model will provide good results. The loss function we use to evaluate the model performance needs to be minimized to improve its performance.\nBroadly speaking, loss functions can be grouped into two major categories concerning the types of problems we come across in the real world: CLASSIFICATION and REGRESSION. In CLASSIFICATION problems, our task is to predict the respective probabilities of all classes the problem is dealing with. When it comes to REGRESSION, our task is to predict the continuous value concerning a given set of independent features to the learning algorithm."
  },
  {
    "objectID": "posts/MLLosses.html#mean-absolute-error-loss",
    "href": "posts/MLLosses.html#mean-absolute-error-loss",
    "title": "Loss and Optimization",
    "section": "1. Mean Absolute Error Loss",
    "text": "1. Mean Absolute Error Loss\nWe define MAE loss function as the average of absolute differences between the actual and the predicted value. It’s the second most commonly used regression loss function. It measures the average magnitude of errors in a set of predictions, without considering their directions.\n\\[MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y_i}|\\]\nwhere \\(y_i\\) is the actual value and \\(\\hat{y_i}\\) is the predicted value.\nThe corresponding cost function is the mean of these absolute errors (MAE). It is also known as the L1 loss function.\n\nimport numpy as np\nimport plotly.graph_objects as go\nimport torch\n\nfrom sklearn.metrics import mean_squared_error, accuracy_score\n\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n&lt;torch._C.Generator at 0x7f07b72789b0&gt;\n\n\n\n# generate data\nx = np.random.uniform(-1, 1, (500, 1))\ny = 2 * x + 3 + np.random.normal(0, 0.5, (500, 1))\n\n# plot data\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=x.flatten(), y=y.flatten(), mode='markers', name='data'))\nfig.update_layout(title='Data', xaxis_title='x', yaxis_title='y')\nfig.show()\n\n\n                                                \n\n\n\n# MAE loss\ndef mae(y, y_pred):\n    return torch.mean(torch.abs(y - y_pred))\n\n\n# add bias term\nX = np.concatenate([x, np.ones((500, 1))], axis=1)\n\n# convert to tensors\nX = torch.from_numpy(X).float()\nY = torch.from_numpy(y).float()\n\n# initialize weights\nw = torch.randn(2, 1, requires_grad=True)\n\nlr = 0.1\nrmse = []\n\n\n# gradient descent\nfor i in range(100):\n    y_pred = torch.matmul(X, w)\n    loss = mae(Y, y_pred)\n    loss.backward()\n    with torch.no_grad():\n        w -= lr * w.grad\n        w.grad.zero_()\n    rmse.append(mean_squared_error(y, y_pred.detach().numpy(), squared=False))\n    \n    if i % 10 == 0:\n        print(f'Epoch {i}, loss {rmse[-1]:.4f}')\n\nEpoch 0, loss 3.2887\nEpoch 10, loss 2.3101\nEpoch 20, loss 1.3679\nEpoch 30, loss 0.6885\nEpoch 40, loss 0.5224\nEpoch 50, loss 0.4988\nEpoch 60, loss 0.4934\nEpoch 70, loss 0.4924\nEpoch 80, loss 0.4924\nEpoch 90, loss 0.4924\n\n\n\n# plot loss\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=np.arange(len(rmse)), y=rmse, mode='lines', name='loss'))\nfig.update_layout(title='Loss', xaxis_title='epoch', yaxis_title='loss')\nfig.show()\n\n# plot data with regression line\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=x[:, 0], y=y.flatten(), mode='markers', name='data'))\nfig.add_trace(go.Scatter(x=x[:, 0], y=2 * x[:, 0] + 3, mode='lines', name='true line', line=dict(color='green')))\nfig.add_trace(go.Scatter(x=x[:, 0], y=y_pred.detach().numpy().flatten(), mode='lines', name='regression line', line=dict(color='red')))\nfig.show()"
  },
  {
    "objectID": "posts/MLLosses.html#mean-squared-error-loss",
    "href": "posts/MLLosses.html#mean-squared-error-loss",
    "title": "Loss and Optimization",
    "section": "2. Mean Squared Error Loss",
    "text": "2. Mean Squared Error Loss\nWe define MSE loss function as the average of squared differences between the actual and the predicted value. It’s the most commonly used regression loss function.\n\\[MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2\\]\nwhere \\(y_i\\) is the actual value and \\(\\hat{y_i}\\) is the predicted value.\nThe corresponding cost function is the mean of these squared errors (MSE). It is also known as the L2 loss function. The MSE loss function penalizes the model for making large errors by squaring them.\n\n# MSE loss\ndef mse(y, y_pred):\n    return torch.mean((y - y_pred) ** 2)\n\n\n# add bias term\nX = np.concatenate([x, np.ones((500, 1))], axis=1)\n\n# convert to tensors\nX = torch.from_numpy(X).float()\nY = torch.from_numpy(y).float()\n\n# initialize weights\nw = torch.randn(2, 1, requires_grad=True)\n\nlr = 0.1\nrmse = []\n\n\n# gradient descent\nfor i in range(100):\n    y_pred = torch.matmul(X, w)\n    loss = mse(Y, y_pred)\n    loss.backward()\n    with torch.no_grad():\n        w -= lr * w.grad\n        w.grad.zero_()\n    rmse.append(mean_squared_error(y, y_pred.detach().numpy(), squared=False))\n    \n    if i % 10 == 0:\n        print(f'Epoch {i}, loss {rmse[-1]:.4f}')\n\nEpoch 0, loss 3.4254\nEpoch 10, loss 1.3340\nEpoch 20, loss 0.7770\nEpoch 30, loss 0.5749\nEpoch 40, loss 0.5136\nEpoch 50, loss 0.4975\nEpoch 60, loss 0.4935\nEpoch 70, loss 0.4925\nEpoch 80, loss 0.4922\nEpoch 90, loss 0.4922\n\n\n\n# plot loss\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=np.arange(len(rmse)), y=rmse, mode='lines', name='loss'))\nfig.update_layout(title='Loss', xaxis_title='epoch', yaxis_title='loss')\nfig.show()\n\n# plot data with regression line\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=x[:, 0], y=y.flatten(), mode='markers', name='data'))\nfig.add_trace(go.Scatter(x=x[:, 0], y=2 * x[:, 0] + 3, mode='lines', name='true line', line=dict(color='green')))\nfig.add_trace(go.Scatter(x=x[:, 0], y=y_pred.detach().numpy().flatten(), mode='lines', name='regression line', line=dict(color='red')))\nfig.update_layout(title='Data', xaxis_title='x', yaxis_title='y')\nfig.show()"
  },
  {
    "objectID": "posts/MLLosses.html#huber-loss",
    "href": "posts/MLLosses.html#huber-loss",
    "title": "Loss and Optimization",
    "section": "3. Huber Loss",
    "text": "3. Huber Loss\nWe define Huber loss function as the combination of MSE and MAE. It’s less sensitive to outliers than the MSE loss function and is differentiable at 0.\n\\[Huber = \\frac{1}{n}\\sum_{i=1}^{n}L_{\\delta}(y_i - \\hat{y_i})\\]\n\\[L_{\\delta}(y_i - \\hat{y_i}) = \\begin{cases} \\frac{1}{2}(y_i - \\hat{y_i})^2 & \\text{for } |y_i - \\hat{y_i}| \\leq \\delta \\\\ \\delta|y_i - \\hat{y_i}| - \\frac{1}{2}\\delta^2 & \\text{otherwise} \\end{cases}\\]\nwhere \\(y_i\\) is the actual value and \\(\\hat{y_i}\\) is the predicted value.\nThe corresponding cost function is the mean of these Huber errors. The Huber loss function is more robust to outliers compared to the MSE loss function.\n\n# Huber loss\ndef huber(y, y_pred, delta=1):\n    abs_diff = torch.abs(y - y_pred)\n    return torch.mean(torch.where(abs_diff &lt; delta, 0.5 * abs_diff ** 2, delta * abs_diff - 0.5 * delta ** 2))\n\n\n# add bias term\nX = np.concatenate([x, np.ones((500, 1))], axis=1)\n\n# convert to tensors\nX = torch.from_numpy(X).float()\nY = torch.from_numpy(y).float()\n\n# initialize weights\nw = torch.randn(2, 1, requires_grad=True)\n\nlr = 0.1\nrmse = []\n\n\n# gradient descent\nfor i in range(100):\n    y_pred = torch.matmul(X, w)\n    loss = huber(Y, y_pred)\n    loss.backward()\n    with torch.no_grad():\n        w -= lr * w.grad\n        w.grad.zero_()\n    rmse.append(mean_squared_error(y, y_pred.detach().numpy(), squared=False))\n    \n    if i % 10 == 0:\n        print(f'Epoch {i}, loss {rmse[-1]:.4f}')\n\nEpoch 0, loss 4.7136\nEpoch 10, loss 3.8319\nEpoch 20, loss 3.0589\nEpoch 30, loss 2.4339\nEpoch 40, loss 1.9426\nEpoch 50, loss 1.5518\nEpoch 60, loss 1.2385\nEpoch 70, loss 0.9916\nEpoch 80, loss 0.8063\nEpoch 90, loss 0.6778\n\n\n\n# plot loss\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=np.arange(len(rmse)), y=rmse, mode='lines', name='loss'))\nfig.update_layout(title='Loss', xaxis_title='epoch', yaxis_title='loss')\nfig.show()\n\n# plot data with regression line\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=x[:, 0], y=y.flatten(), mode='markers', name='data'))\nfig.add_trace(go.Scatter(x=x[:, 0], y=2 * x[:, 0] + 3, mode='lines', name='true line', line=dict(color='green')))\nfig.add_trace(go.Scatter(x=x[:, 0], y=y_pred.detach().numpy().flatten(), mode='lines', name='regression line', line=dict(color='red')))\nfig.show()"
  },
  {
    "objectID": "posts/MLLosses.html#binary-cross-entropy-loss",
    "href": "posts/MLLosses.html#binary-cross-entropy-loss",
    "title": "Loss and Optimization",
    "section": "1. Binary Cross-Entropy Loss",
    "text": "1. Binary Cross-Entropy Loss\nThis is the most common loss function used in classification problems. The binary cross-entropy loss decreases as the predicted probability converges to the actual label. It measures the performance of a classification model whose predicted output is a probability value between 0 and 1.\n\\[L = \\begin{cases} -log(\\hat{y_i}) & \\text{if } y_i = 1 \\\\ -log(1-\\hat{y_i}) & \\text{if } y_i = 0 \\end{cases}\\]\n\\[L = - \\dfrac{1}{m} \\sum_{i=1}^{m} y_i \\log(\\hat{y_i}) + (1-y_i) \\log(1-\\hat{y_i})\\]\nwhere \\(y_i\\) is the actual value and \\(\\hat{y_i}\\) is the predicted value.\n\nfrom sklearn.datasets import make_blobs\n\n\n# generate data blobs\nx, y = make_blobs(n_samples=500, centers=2, cluster_std=2, random_state=42)\n\ncolor = np.where(y == 0.0, 'orange', 'blue')\n\n# plot data\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=x[:, 0], y=x[:, 1], mode='markers', marker=dict(color=color)))\nfig.update_layout(title='Data', xaxis_title='x', yaxis_title='y')\nfig.show()\n\n\n                                                \n\n\n\ndef bce(y, y_pred):\n    ce = -torch.mean(y * torch.log(y_pred) + (1 - y) * torch.log(1 - y_pred))\n    return ce\n\n\n# add bias term\nX = np.concatenate([x, np.ones((500, 1))], axis=1)\n\n# convert to tensors\nX = torch.from_numpy(X).float()\nY = torch.from_numpy(y).float()\n\n# initialize weights\nw = torch.randn(3, 1, requires_grad=True)\n\nlr = 0.01\naccuracy = []\n\n\n# gradient descent\nfor i in range(100):\n    y_pred = torch.matmul(X, w)\n    y_pred = torch.sigmoid(y_pred)\n    loss = bce(Y, y_pred)\n    loss.backward()\n    with torch.no_grad():\n        w -= lr * w.grad\n        w.grad.zero_()\n    \n    y_pred = torch.where(y_pred &gt; 0.5, 1.0, 0.0)\n    accuracy.append(accuracy_score(y, y_pred.detach().numpy()))\n    \n    if i % 10 == 0:\n        print(f'Epoch {i}, loss {loss:.4f}, accuracy {accuracy[-1]:.4f}')\n\nEpoch 0, loss 0.9751, accuracy 0.5820\nEpoch 10, loss 0.7332, accuracy 0.5260\nEpoch 20, loss 0.6994, accuracy 0.5560\nEpoch 30, loss 0.6967, accuracy 0.4740\nEpoch 40, loss 0.6965, accuracy 0.4380\nEpoch 50, loss 0.6964, accuracy 0.4300\nEpoch 60, loss 0.6964, accuracy 0.4300\nEpoch 70, loss 0.6964, accuracy 0.4280\nEpoch 80, loss 0.6964, accuracy 0.4260\nEpoch 90, loss 0.6963, accuracy 0.4260\n\n\n\n# plot loss\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=np.arange(len(accuracy)), y=accuracy, mode='lines', name='accuracy'))\nfig.update_layout(title='Accuracy', xaxis_title='epoch', yaxis_title='accuracy')\nfig.show()\n\n# plot data with regression line\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=x[:, 0], y=x[:, 1], mode='markers', marker=dict(color=color)))\nfig.add_trace(go.Scatter(x=x[:, 0], y=(-(w[0] * X[:, 0] + w[2]) / w[1]).detach().numpy(), mode='lines', name='regression line', line=dict(color='red')))\nfig.show()"
  },
  {
    "objectID": "posts/MLLosses.html#focal-loss",
    "href": "posts/MLLosses.html#focal-loss",
    "title": "Loss and Optimization",
    "section": "2. Focal Loss",
    "text": "2. Focal Loss\nWe define Focal loss function as the combination of Binary Cross-Entropy Loss and a modulating factor. The modulating factor \\(\\gamma\\) is used to reduce the relative loss for well-classified examples and put more focus on hard, misclassified examples. It’s less sensitive to outliers than the Binary Cross-Entropy Loss function and is differentiable at 0.\n\\[FL = \\begin{cases} -(1-\\hat{y_i})^{\\gamma}log(\\hat{y_i}) & \\text{if } y_i = 1 \\\\ -(\\hat{y_i})^{\\gamma}log(1-\\hat{y_i}) & \\text{if } y_i = 0 \\end{cases}\\]\n\\[FL = - \\dfrac{1}{m} \\sum_{i=1}^{m} y_i (1 - \\hat{y_i})^{\\gamma} \\log(\\hat{y_i}) + (1-y_i) (\\hat{y_i})^{\\gamma} \\log(1-\\hat{y_i})\\]\nIn practice, we use an \\(\\alpha\\)-balanced variant of the focal loss that inherits the characteristics of both the weighing factor \\(\\alpha\\) and the focusing parameter \\(\\gamma\\), yielding slightly better accuracy than the non-balanced form.\n\\[ FL = \\begin{cases} -\\alpha(1-\\hat{y_i})^{\\gamma}log(\\hat{y_i}) & \\text{if } y_i = 1 \\\\ -(1-\\alpha)(\\hat{y_i})^{\\gamma}log(1-\\hat{y_i}) & \\text{if } y_i = 0 \\end{cases}\\]\n\\[ FL = - \\dfrac{1}{m} \\sum_{i=1}^{m} y_i \\alpha (1 - \\hat{y_i})^{\\gamma} \\log(\\hat{y_i}) + (1-y_i) (1-\\alpha) (\\hat{y_i})^{\\gamma} \\log(1-\\hat{y_i})\\]\nwhere \\(y_i\\) is the actual label and \\(\\hat{y_i}\\) is the predicted probability of the label.\n\n# Focal Loss\ndef focal_loss(y, y_pred, alpha=1, gamma=2):\n    bce_loss = bce(y, y_pred) #-torch.mean(y * torch.log(y_pred) + (1 - y) * torch.log(1 - y_pred))\n    pt = torch.exp(-bce_loss)\n    return alpha * (1 - pt) ** gamma * bce_loss\n\n\n# add bias term\nX = np.concatenate([x, np.ones((500, 1))], axis=1)\n\n# convert to tensors\nX = torch.from_numpy(X).float()\nY = torch.from_numpy(y).float()\n\n# initialize weights\nw = torch.randn(3, 1, requires_grad=True)\n\nlr = 0.1\naccuracy = []\n\n\n# gradient descent\nfor i in range(100):\n    y_pred = torch.matmul(X, w)\n    loss = focal_loss(Y, torch.sigmoid(y_pred))\n    loss.backward()\n    with torch.no_grad():\n        w -= lr * w.grad\n        w.grad.zero_()\n    accuracy.append(1 - accuracy_score(y, [1 if i &gt; 0.5 else 0 for i in torch.sigmoid(y_pred).detach().numpy()]))\n    \n    if i % 10 == 0:\n        print(f'Epoch {i}, accuracy {accuracy[-1]:.4f}')\n\nEpoch 0, accuracy 0.4980\nEpoch 10, accuracy 0.3460\nEpoch 20, accuracy 0.4220\nEpoch 30, accuracy 0.4260\nEpoch 40, accuracy 0.4280\nEpoch 50, accuracy 0.4280\nEpoch 60, accuracy 0.4280\nEpoch 70, accuracy 0.4280\nEpoch 80, accuracy 0.4280\nEpoch 90, accuracy 0.4280\n\n\n\n# plot loss\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=np.arange(len(accuracy)), y=accuracy, mode='lines', name='loss'))\nfig.update_layout(title='Accuracy', xaxis_title='epoch', yaxis_title='loss')\nfig.show()\n\n# plot data with regression line\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=x[:, 0], y=x[:, 1], mode='markers', marker=dict(color=color)))\nfig.add_trace(go.Scatter(x=x[:, 0], y=(-(w[0] * X[:, 0] + w[2]) / w[1]).detach().numpy(), mode='lines', name='regression line', line=dict(color='red')))\nfig.show()"
  },
  {
    "objectID": "posts/Probablity_distibution.html",
    "href": "posts/Probablity_distibution.html",
    "title": "Probability",
    "section": "",
    "text": "Bernoulli distribution\n\n\nBernoulli distribution is a discret univariate probability distribution. A Bernoulli trial or experiment results in binary outcomes: success or failure \\((0 or 1)\\). The trial’s success is denoted as $ p (x=1)$, and failure is expressed as \\(1-p ( x=0)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{equation}\nP(X = x) = p^x \\cdot (1-p)^{1-x} \\tag{1}\n\\end{equation}\\]\nwhere \\((X)\\) is the random variable, \\((x)\\) can be either 0 or 1, and \\((p)\\) is the probability of success.\nSome imports\n\nimport numpy as np\nimport torch\nfrom torch.distributions import Bernoulli\nimport math\nfrom torch.optim import Adam\nimport matplotlib.pyplot as plt\n\nImplementation of PMF\n\np=0.6 #success=0.6 failure=0.4\nimport numpy as np\nsample = np.random.choice([0, 1], p=[1 - p, p])\nprob = (p ** sample) * ((1 - p) ** (1 - sample))\nprint(\"Sample:\", sample)\nprint(\"Probability:\", prob)\n\n#Using PyTorch\nimport torch\nfrom torch.distributions import Bernoulli\ndist=Bernoulli(torch.tensor([p]))\nsample=dist.sample()\nprint(\"Sample:\", sample)\nprint(\"Probability:\", dist)\n\n#Set of Probablty of success\nprobs = torch.tensor([0.7, 0.4, 0.9])\nbernoulli_dist = Bernoulli(probs=probs,logits=None)\nsamples = bernoulli_dist.sample()\nprint(\"probablity distributions:\", bernoulli_dist)\nprint(\"Samples:\", samples)\n\n# Log-odds of success\nlogits = torch.tensor([0.847])\ndist = Bernoulli(probs=None,logits=logits)\nsample = dist.sample()\nprint(\"log odd prob :\", dist)\nprint(\"Sample:\", sample.item())\n\nSample: 0\nProbability: 0.4\nSample: tensor([1.])\nProbability: Bernoulli(probs: tensor([0.6000]))\nprobablity distributions: Bernoulli(probs: torch.Size([3]))\nSamples: tensor([1., 0., 1.])\nlog odd prob : Bernoulli(probs: tensor([0.6999]), logits: tensor([0.8470]))\nSample: 1.0\n\n\nLog probability of Bernoulli distribution\nTo obtain the log probability, we take the natural logarithm of the PMF:\n\\[\\begin{equation}\n\\log P(X=x) = \\log(p^x \\cdot (1-p)^{1-x})\n\\end{equation}\\]\n\\[\\begin{equation}\n\\log P(X=x) = x \\cdot \\log(p) + (1-x) \\cdot \\log(1-p)\n\\end{equation}\\]\n\nsample=1\nprob=0.6\nlog_probability = sample * math.log(p) + (1 - sample) * math.log(1 - p)\nprint(\"sample:\", sample)\nprint(\"Log Probability:\", log_probability)\n\n#using PyTorch\nsample = torch.tensor([1])\np = torch.tensor([0.6])\ndist = Bernoulli(probs=p,logits=None)\nsample=dist.sample()\nlog_prob=dist.log_prob(sample)\nprint(\"Sample:\", sample)\nprint(\"Log Probability:\", log_prob)\n\nsample: 1\nLog Probability: -0.5108256237659907\nSample: tensor([1.])\nLog Probability: tensor([-0.5108])\n\n\nMaximum Likelihood Estimations(MLE) for Bernoulli Distribution\nTo derive the Maximum Likelihood Estimation (MLE) for the Bernoulli distribution, let’s assume we have a random sample of independent and identically distributed (i.i.d.) observations from a Bernoulli distribution with parameter p. Each observation can take a value of either 0 or 1.\nThe likelihood function for the Bernoulli distribution is given by:\n\\[ L(p) = \\prod_{i=1}^{n} p^{x_i} \\cdot (1-p)^{1-x_i} \\]\nwhere (x_i) is the i-th observation in the sample and n is the total number of observations.\nTo find the MLE for p, we want to find the value of p that maximizes the likelihood function L(p). It is often easier to work with the log-likelihood function, which is the natural logarithm of the likelihood function:\n\\[\n\\log L(p) = \\sum_{i=1}^{n} x_i \\cdot \\log(p) + (1-x_i) \\cdot \\log(1-p)\n\\]\nTo find the MLE, we differentiate the log-likelihood function with respect to p and set it equal to zero:\n\\[\n\\frac{d}{dp}(\\log L(p)) = \\frac{1}{p}\\sum_{i=1}^{n} x_i - \\frac{1}{1-p}\\sum_{i=1}^{n} (1-x_i) = 0\n\\]\nSimplifying the equation:\n\\[\n\\frac{1}{p}\\sum_{i=1}^{n} x_i - \\frac{n}{1-p} + \\frac{1}{1-p}\\sum_{i=1}^{n} x_i = 0\n\\]\nMultiplying through by p(1-p):\n\\[\n(1-p)\\sum_{i=1}^{n} x_i - np + p\\sum_{i=1}^{n} x_i = 0\n\\]\nRearranging the terms:\n\\[\n\\sum_{i=1}^{n} x_i - np = 0\n\\] Finally, solving for p:\n\\[\np = \\frac{1}{n}\\sum_{i=1}^{n} x_i\n\\]\nTherefore, the MLE for the parameter p in the Bernoulli distribution is the sample mean of the observed values.\nIt is important to note that this MLE is consistent, unbiased, and efficient for estimating the parameter p in the Bernoulli distribution.\n\nsize = 100\ndataset = dist.sample(torch.Size([size]))\nnum_suc=dataset.float().sum()\np_estimate=num_suc.float()/dataset.size(0)\nprint(\"MLE Estimate:\", p_estimate.item())\n\nMLE Estimate: 0.6299999952316284\n\n\nPerforming Maximum Likelihood Estimation (MLE) for the Bernoulli distribution with varying dataset sizes. It computes the negative log-likelihood loss for different dataset sizes and optimizes the parameter ‘p’ to minimize the loss using the Adam optimizer.\nThe resulting loss values are then plotted against the iterations to visualize the convergence of the MLE estimation.\n\ndataset_sizes = [10, 50, 100, 200, 500,1000,10000]\ndef negative_log_likelihood(p, dataset):\n    return -(dataset * torch.log(p) + (1 - dataset) * torch.log(1 - p)).mean()\n\nfor size in dataset_sizes:\n   \n    dataset = torch.randint(low=0, high=2, size=(size,))\n    p = torch.tensor(0.5, requires_grad=True)\n    optimizer = Adam([p], lr=0.1)\n    loss_values = []\n    iteration_values = []\n    for i in range(100):\n        optimizer.zero_grad()\n        loss = negative_log_likelihood(p, dataset)\n        loss.backward()\n        optimizer.step()\n        loss_values.append(loss.item())\n        iteration_values.append(i+1)\n    plt.plot(iteration_values, loss_values, label=f'Dataset Size: {size}')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Loss vs. Iteration for Varying Dataset Sizes')\nplt.legend()\nplt.show()\n\n\n\n\nThe plot shows the relationship between the loss and the number of iterations for each dataset size.\nBy examining the plot, we can observe the following:\n\nAs the dataset size increases, the convergence to the optimal parameter value tends to be faster. This is because larger datasets provide more information, allowing for more accurate estimation.\nFor smaller dataset sizes (e.g., 10, 50, 100), the loss tends to fluctuate more initially. However, as the number of iterations increases, the loss converges to a stable value.\nFor larger dataset sizes (e.g., 1000, 10000), the loss tends to converge quickly and stabilize earlier compared to smaller dataset sizes."
  },
  {
    "objectID": "posts/Probablity_distibution.html#probablity-distribution",
    "href": "posts/Probablity_distibution.html#probablity-distribution",
    "title": "Probability",
    "section": "",
    "text": "Bernoulli distribution\n\n\nBernoulli distribution is a discret univariate probability distribution. A Bernoulli trial or experiment results in binary outcomes: success or failure \\((0 or 1)\\). The trial’s success is denoted as $ p (x=1)$, and failure is expressed as \\(1-p ( x=0)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{equation}\nP(X = x) = p^x \\cdot (1-p)^{1-x} \\tag{1}\n\\end{equation}\\]\nwhere \\((X)\\) is the random variable, \\((x)\\) can be either 0 or 1, and \\((p)\\) is the probability of success.\nSome imports\n\nimport numpy as np\nimport torch\nfrom torch.distributions import Bernoulli\nimport math\nfrom torch.optim import Adam\nimport matplotlib.pyplot as plt\n\nImplementation of PMF\n\np=0.6 #success=0.6 failure=0.4\nimport numpy as np\nsample = np.random.choice([0, 1], p=[1 - p, p])\nprob = (p ** sample) * ((1 - p) ** (1 - sample))\nprint(\"Sample:\", sample)\nprint(\"Probability:\", prob)\n\n#Using PyTorch\nimport torch\nfrom torch.distributions import Bernoulli\ndist=Bernoulli(torch.tensor([p]))\nsample=dist.sample()\nprint(\"Sample:\", sample)\nprint(\"Probability:\", dist)\n\n#Set of Probablty of success\nprobs = torch.tensor([0.7, 0.4, 0.9])\nbernoulli_dist = Bernoulli(probs=probs,logits=None)\nsamples = bernoulli_dist.sample()\nprint(\"probablity distributions:\", bernoulli_dist)\nprint(\"Samples:\", samples)\n\n# Log-odds of success\nlogits = torch.tensor([0.847])\ndist = Bernoulli(probs=None,logits=logits)\nsample = dist.sample()\nprint(\"log odd prob :\", dist)\nprint(\"Sample:\", sample.item())\n\nSample: 0\nProbability: 0.4\nSample: tensor([1.])\nProbability: Bernoulli(probs: tensor([0.6000]))\nprobablity distributions: Bernoulli(probs: torch.Size([3]))\nSamples: tensor([1., 0., 1.])\nlog odd prob : Bernoulli(probs: tensor([0.6999]), logits: tensor([0.8470]))\nSample: 1.0\n\n\nLog probability of Bernoulli distribution\nTo obtain the log probability, we take the natural logarithm of the PMF:\n\\[\\begin{equation}\n\\log P(X=x) = \\log(p^x \\cdot (1-p)^{1-x})\n\\end{equation}\\]\n\\[\\begin{equation}\n\\log P(X=x) = x \\cdot \\log(p) + (1-x) \\cdot \\log(1-p)\n\\end{equation}\\]\n\nsample=1\nprob=0.6\nlog_probability = sample * math.log(p) + (1 - sample) * math.log(1 - p)\nprint(\"sample:\", sample)\nprint(\"Log Probability:\", log_probability)\n\n#using PyTorch\nsample = torch.tensor([1])\np = torch.tensor([0.6])\ndist = Bernoulli(probs=p,logits=None)\nsample=dist.sample()\nlog_prob=dist.log_prob(sample)\nprint(\"Sample:\", sample)\nprint(\"Log Probability:\", log_prob)\n\nsample: 1\nLog Probability: -0.5108256237659907\nSample: tensor([1.])\nLog Probability: tensor([-0.5108])\n\n\nMaximum Likelihood Estimations(MLE) for Bernoulli Distribution\nTo derive the Maximum Likelihood Estimation (MLE) for the Bernoulli distribution, let’s assume we have a random sample of independent and identically distributed (i.i.d.) observations from a Bernoulli distribution with parameter p. Each observation can take a value of either 0 or 1.\nThe likelihood function for the Bernoulli distribution is given by:\n\\[ L(p) = \\prod_{i=1}^{n} p^{x_i} \\cdot (1-p)^{1-x_i} \\]\nwhere (x_i) is the i-th observation in the sample and n is the total number of observations.\nTo find the MLE for p, we want to find the value of p that maximizes the likelihood function L(p). It is often easier to work with the log-likelihood function, which is the natural logarithm of the likelihood function:\n\\[\n\\log L(p) = \\sum_{i=1}^{n} x_i \\cdot \\log(p) + (1-x_i) \\cdot \\log(1-p)\n\\]\nTo find the MLE, we differentiate the log-likelihood function with respect to p and set it equal to zero:\n\\[\n\\frac{d}{dp}(\\log L(p)) = \\frac{1}{p}\\sum_{i=1}^{n} x_i - \\frac{1}{1-p}\\sum_{i=1}^{n} (1-x_i) = 0\n\\]\nSimplifying the equation:\n\\[\n\\frac{1}{p}\\sum_{i=1}^{n} x_i - \\frac{n}{1-p} + \\frac{1}{1-p}\\sum_{i=1}^{n} x_i = 0\n\\]\nMultiplying through by p(1-p):\n\\[\n(1-p)\\sum_{i=1}^{n} x_i - np + p\\sum_{i=1}^{n} x_i = 0\n\\]\nRearranging the terms:\n\\[\n\\sum_{i=1}^{n} x_i - np = 0\n\\] Finally, solving for p:\n\\[\np = \\frac{1}{n}\\sum_{i=1}^{n} x_i\n\\]\nTherefore, the MLE for the parameter p in the Bernoulli distribution is the sample mean of the observed values.\nIt is important to note that this MLE is consistent, unbiased, and efficient for estimating the parameter p in the Bernoulli distribution.\n\nsize = 100\ndataset = dist.sample(torch.Size([size]))\nnum_suc=dataset.float().sum()\np_estimate=num_suc.float()/dataset.size(0)\nprint(\"MLE Estimate:\", p_estimate.item())\n\nMLE Estimate: 0.6299999952316284\n\n\nPerforming Maximum Likelihood Estimation (MLE) for the Bernoulli distribution with varying dataset sizes. It computes the negative log-likelihood loss for different dataset sizes and optimizes the parameter ‘p’ to minimize the loss using the Adam optimizer.\nThe resulting loss values are then plotted against the iterations to visualize the convergence of the MLE estimation.\n\ndataset_sizes = [10, 50, 100, 200, 500,1000,10000]\ndef negative_log_likelihood(p, dataset):\n    return -(dataset * torch.log(p) + (1 - dataset) * torch.log(1 - p)).mean()\n\nfor size in dataset_sizes:\n   \n    dataset = torch.randint(low=0, high=2, size=(size,))\n    p = torch.tensor(0.5, requires_grad=True)\n    optimizer = Adam([p], lr=0.1)\n    loss_values = []\n    iteration_values = []\n    for i in range(100):\n        optimizer.zero_grad()\n        loss = negative_log_likelihood(p, dataset)\n        loss.backward()\n        optimizer.step()\n        loss_values.append(loss.item())\n        iteration_values.append(i+1)\n    plt.plot(iteration_values, loss_values, label=f'Dataset Size: {size}')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Loss vs. Iteration for Varying Dataset Sizes')\nplt.legend()\nplt.show()\n\n\n\n\nThe plot shows the relationship between the loss and the number of iterations for each dataset size.\nBy examining the plot, we can observe the following:\n\nAs the dataset size increases, the convergence to the optimal parameter value tends to be faster. This is because larger datasets provide more information, allowing for more accurate estimation.\nFor smaller dataset sizes (e.g., 10, 50, 100), the loss tends to fluctuate more initially. However, as the number of iterations increases, the loss converges to a stable value.\nFor larger dataset sizes (e.g., 1000, 10000), the loss tends to converge quickly and stabilize earlier compared to smaller dataset sizes."
  },
  {
    "objectID": "posts/thuplots.html",
    "href": "posts/thuplots.html",
    "title": "Tue Plots",
    "section": "",
    "text": "import torch\nimport torch.autograd.functional as F\nimport torch.distributions as dist\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.sans-serif'] = ['Liberation Sans']\n\n\n\nimport pandas as pd\n%matplotlib inline\n\n\nfrom tueplots import bundles\nplt.rcParams.update(bundles.beamer_moml())\n#plt.rcParams.update(bundles.icml2022())\n\n\n# Also add despine to the bundle using rcParams\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\n\n# Increase font size to match Beamer template\nplt.rcParams['font.size'] = 16\n# Make background transparent\nplt.rcParams['figure.facecolor'] = 'none'\n\n\ntry:\n    import hamiltorch\nexcept ImportError:\n    %pip install git+https://github.com/AdamCobb/hamiltorch\n\n\nhamiltorch.set_random_seed(123)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\n\ndevice(type='cpu')\n\n\n\ngt_distribution = torch.distributions.Normal(0, 1)\n\n# Samples from the ground truth distribution\ndef sample_gt(n):\n    return gt_distribution.sample((n,))\n\nsamples = sample_gt(1000)\n\n\nx_lin = torch.linspace(-3, 3, 1000)\ny_lin = torch.exp(gt_distribution.log_prob(x_lin))\n\nplt.plot(x_lin, y_lin, label='Ground truth')\n\n\n\n\n\n# Logprob function to be passed to Hamiltorch sampler\ndef logprob(x):\n    return gt_distribution.log_prob(x).sum()\n\n# Initial state\nx0 = torch.tensor([0.0])\nnum_samples = 5000\nstep_size = 0.3\nnum_steps_per_sample = 5\nhamiltorch.set_random_seed(123)\n\n\nparams_hmc = hamiltorch.sample(log_prob_func=logprob, params_init=x0,  \n                               num_samples=num_samples, step_size=step_size, \n                               num_steps_per_sample=num_steps_per_sample)\n\nSampling (Sampler.HMC; Integrator.IMPLICIT)\nTime spent  | Time remain.| Progress             | Samples   | Samples/sec\n0d:00:00:16 | 0d:00:00:00 | #################### | 5000/5000 | 308.91       \nAcceptance Rate 0.99\n\n\n\nparams_hmc = torch.tensor(params_hmc)\n# Trace plot\nplt.plot(params_hmc, label='Trace')\nplt.xlabel('Iteration')\nplt.ylabel('Parameter value')\n\nText(0, 0.5, 'Parameter value')\n\n\n\n\n\n\n# Logprob function to be passed to Hamiltorch sampler\ndef logprob(x):\n    return gt_distribution.log_prob(x).sum()\n\n# Initial state\nx0 = torch.tensor([0.0])\nnum_samples = 5000\nstep_size = 0.3\nnum_steps_per_sample = 5\nhamiltorch.set_random_seed(123)\n\n\nparams_hmc = hamiltorch.sample(log_prob_func=logprob, params_init=x0,  \n                               num_samples=num_samples, step_size=step_size, \n                               num_steps_per_sample=num_steps_per_sample)\n\nSampling (Sampler.HMC; Integrator.IMPLICIT)\nTime spent  | Time remain.| Progress             | Samples   | Samples/sec\n0d:00:00:14 | 0d:00:00:00 | #################### | 5000/5000 | 338.09       \nAcceptance Rate 0.99\n\n\n\nparams_hmc = torch.tensor(params_hmc)\n# Trace plot\nplt.plot(params_hmc, label='Trace')\nplt.xlabel('Iteration')\nplt.ylabel('Parameter value')\n\n/tmp/ipykernel_18135/1433682485.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  params_hmc = torch.tensor(params_hmc)\n\n\nText(0, 0.5, 'Parameter value')\n\n\n\n\n\n\n# KDE plot\nimport seaborn as sns\nplt.figure()\nsns.kdeplot(params_hmc.detach().numpy(), label='Samples', shade=True, color='C1')\nplt.plot(x_lin, y_lin, label='Ground truth')\nplt.xlabel('Parameter value')\nplt.ylabel('Density')\nplt.legend()\n\n/tmp/ipykernel_18135/469715340.py:4: FutureWarning: \n\n`shade` is now deprecated in favor of `fill`; setting `fill=True`.\nThis will become an error in seaborn v0.14.0; please update your code.\n\n  sns.kdeplot(params_hmc.detach().numpy(), label='Samples', shade=True, color='C1')\n\n\n&lt;matplotlib.legend.Legend at 0x7f51dfedd4b0&gt;\n\n\n\n\n\n\n# Linear regression for 1 dimensional input using HMC\n\nx_lin = torch.linspace(-3, 3, 90)\ntheta_0_true = torch.tensor([2.0])\ntheta_1_true = torch.tensor([3.0])\nf = lambda x: theta_0_true + theta_1_true * x\neps = torch.randn_like(x_lin) *1.0\ny_lin = f(x_lin) + eps\n\nplt.scatter(x_lin, y_lin, label='Data', color='C0')\nplt.plot(x_lin, f(x_lin), label='Ground truth')\nplt.xlabel('x')\nplt.ylabel('y')\n\nText(0, 0.5, 'y')"
  },
  {
    "objectID": "posts/Sampling_distributions.html",
    "href": "posts/Sampling_distributions.html",
    "title": "Distribution",
    "section": "",
    "text": "This blog is your gateway to delve into various commonly used univariate sampling. Here, you will find intriguing insights into the stories behind these distributions. For example, you will discover that the outcome of a coin flip follows a Bernoulli distribution. We provide comprehensive information about each distribution, including their probability mass or probability density functions, moments, and more. Additionally, we offer implementations of these distributions using PyTorch, allowing you to explore and experiment with different flavors and variations of each distribution."
  },
  {
    "objectID": "posts/Sampling_distributions.html#univariate-discrete-distributions",
    "href": "posts/Sampling_distributions.html#univariate-discrete-distributions",
    "title": "Distribution",
    "section": "Univariate discrete distributions",
    "text": "Univariate discrete distributions\n\nDiscrete uniform distribution\n****Definition**** : Let \\(X\\) be a discrete rv. Then \\(X\\) is said to be uniformly distributed with minimum \\(a\\) and maximum \\(b\\)\n\\[X \\sim U(a,b)\\]\n****Support**** : \\((a,b)\\)\n****pmf plot****\n\ndef pmf_plot(x,y):\n    plt.figure(figsize=(6, 4))\n    y = pmf_fn(x)\n    plt.bar(x, y)\n    plt.ylabel(\"pmf\")\n    plt.xlabel(\"support\")\n    plt.show()\n\n****pdf plot****\n\ndef pdf_plot(x,y):\n    plt.figure(figsize=(6, 4))\n    y = pdf_fn(x)\n    plt.plot(x, y)\n    plt.ylabel(\"pdf\")\n    plt.xlabel(\"support\")\n    plt.show()\n\n****cdf plot****\n\ndef cdf_plot(x,y):\n    plt.figure(figsize=(6, 4))\n    y = cdf_fn(x)\n    plt.plot(x, y)\n    plt.ylabel(\"cdf\")\n    plt.xlabel(\"support\")\n    plt.show()\n\n****inverse cdf plot****\n\ndef icdf_plot(x,y):\n    plt.figure(figsize=(6, 4))\n    y = icdf_fn(x)\n    plt.plot(x, y)\n    plt.ylabel(\"inverse cdf\")\n    plt.xlabel(\"distribution\")\n    plt.show()\n\n\n\nBernoulli distribution\nDefinition : Let \\(X\\) be a random variable. Then, \\(X\\) is said to follow a Bernoulli distribution with success probability \\(p\\) \\[X \\sim Bern(p)\\]\nPMF: \\[\nf(x, p) = \\begin{cases}\np & \\text{if } x = 1 \\\\\n1 - p & \\text{if } x = 0 \\\\\n\\end{cases}\n\\]\nPMF of bernoulli distribution\n\n\nfrom torch.distributions import Bernoulli\np = torch.tensor(0.4)\nbernoulli = Bernoulli(probs=p)\nbernoulli_samples = bernoulli.sample((1000,))\nx = [0, 1]\nf = []\ns = []\nfor i in bernoulli_samples:\n    if i == 1:\n        s.append(i)\n    else:\n        f.append(i)\ncategories = ['0', '1']\npmf = [len(f) / len(bernoulli_samples), len(s) / len(bernoulli_samples)]\nplt.bar(categories, pmf)\nplt.ylabel(\"Probability\")\nplt.xlabel(\"Support\")\n\nText(0.5, 0, 'Support')\n\n\n\n\n\nCDF:\\[{\\begin{cases}0&{\\text{if }}k&lt;0\\\\1-p&{\\text{if }}0\\leq k&lt;1\\\\1&{\\text{if }}k\\geq 1\\end{cases}}\\]\nCDF of Bernoulli distribution\n\np = 0.4\nbernoulli_dist = dist.Bernoulli(probs=torch.tensor(p))\nx = torch.tensor([0, 1]) \ncdf = torch.tensor([1.0 - p,1])\nplt.bar(x, cdf)\nplt.xlabel('support')\nplt.ylabel('cdf')\n\nText(0, 0.5, 'cdf')\n\n\n\n\n\n\n\nCategorical distribution\nDefinition - In a set of discrete outcomes, each outcome is assigned a probability. \\[X∼Cat([p_1,…,p_k])\\]\n****pmf**** : \\[\nf(x; p_1, p_2, ..., p_k) =\n\\begin{cases}\np_1 & \\text{if } x = 1 \\\\\np_2 & \\text{if } x = 2 \\\\\n\\vdots \\\\\np_k & \\text{if } x = k \\\\\n\\end{cases}\n\\]\nPMF of Categorical distribution\n\nfrom torch.distributions import Categorical\nprobs=torch.tensor([0.20,0.40,0.40])\ncategorical_distribution = Categorical(probs)\ncategorical_numbers = categorical_distribution.sample((1000,))\ncategory_counts = torch.bincount(categorical_numbers) #Bincount:count the number of occurrences of each value                                                       #of occurrences of each value\nprobabilities = category_counts / 1000\ncategories = torch.arange(1,len(probabilities)+1)\nplt.bar(categories, probabilities)\nplt.ylabel(\"probabilities\")\nplt.xlabel(\"categories\")\n\nText(0.5, 0, 'categories')\n\n\n\n\n\nCDF of Categorical distribution\n\nprobs=torch.tensor([.20,.40,.40])\ncdf = torch.cumsum((probs),dim=0)\nprint(cdf)\ncategories = torch.arange(1,len(probabilities)+1)\nplt.bar(categories,cdf)\nplt.ylabel(\"cdf\")\nplt.xlabel(\"categories\")\n\ntensor([0.2000, 0.6000, 1.0000])\n\n\nText(0.5, 0, 'categories')"
  },
  {
    "objectID": "posts/Sampling_distributions.html#univariate-continuous-distributions",
    "href": "posts/Sampling_distributions.html#univariate-continuous-distributions",
    "title": "Distribution",
    "section": "Univariate continuous distributions",
    "text": "Univariate continuous distributions\nPDF : \\[f_X(x) = \\frac{1}{b - a}, \\quad \\text{where } x \\in \\{a, a+1, \\ldots, b-1, b\\}\\]\n\nfrom torch.distributions import Uniform\nx = torch.linspace(2,6,1000)\ndef pdf_fn(x):\n    a=2\n    b=6\n    uniform_dist = Uniform(a, b)\n    return uniform_dist.log_prob(x).exp()\npdf_plot(x,pdf_fn(x))\n\n\n\n\nCDF: \\[F(x) = \\begin{cases}\n0 & \\text{if } x &lt; a \\\\\n\\frac{{x - a}}{{b - a}} & \\text{if } a \\leq x &lt; b \\\\\n1 & \\text{if } x \\geq b \\\\\n\\end{cases}\n\\]\n\na=1\nb=6\nx=torch.linspace(1,6,10)\ndef cdf_fn(x):\n    return (x - a) / (b - a)\ncdf_plot(x,cdf_fn(x))\n\n\n\n\n\nNormal distribution\nDefinition : The normal distribution arises when many small factors contribute to a quantity without any extreme variations, resulting in a bell-shaped curve.\nPDF :\n\\[\nf(x;\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n\\]\nSupport : (\\(-\\infty\\) to \\(+\\infty\\))\n\nfrom torch.distributions import Normal\na = -5\nb = 5\nx = torch.linspace(a,b,1000)\ndef pdf_fn(x):\n    mean = torch.tensor([0.0])\n    stddev = torch.tensor([1.0])\n    normal_dist = dist.Normal(mean, stddev)\n    return torch.exp(normal_dist.log_prob(x))\nplt.plot(x, pdf_fn(x))\n\n\n\n\n\n****CDF**** : \\[F_X(x) = \\frac{1}{2} \\left[ 1 + \\text{erf}\\left(\\frac{x - \\mu}{\\sqrt{2}\\sigma}\\right) \\right]\\]\nCDF of Normal distribution\n\nfrom scipy.stats import norm\na = -5\nb = 5\nx = torch.linspace(a,b,1000)\ndef cdf_fn(x):\n    return norm.cdf(x, loc=0, scale=1)\ncdf_plot(x,cdf_fn(x))\n\n\n\n\nInverse CDF: \\[Q_X(p) = \\sqrt{2\\sigma} \\cdot \\text{erf}^{-1}(2p - 1) + \\mu\\]\nInverse CDF of Normal distribution\n\na = 0.01\nb = 0.99\nx = torch.linspace(a,b,1000)\ndef icdf_fn(x):\n    return norm.ppf(x, loc=0, scale=1)\nicdf_plot(x,icdf_fn(x))\n\n\n\n\n\nplt.plot(x, pdf_fn(x),linestyle = 'dashed')\nicdf_samples=icdf_fn(x)\nsns.kdeplot(icdf_samples, color='green',shade='True')\nplt.xlabel(\"x\")\nplt.ylabel(\"Density\")\nplt.title(\"Kernel Density Estimation\")\n\nText(0.5, 1.0, 'Kernel Density Estimation')\n\n\n\n\n\n\n\nBeta distribution\nDefinition : Let’s say you have two processes, each consisting of multiple steps. Both processes occur at the same rate, but the first process requires \\(\\alpha\\) step and the second process \\(\\beta\\) ,the fraction of the total waiting time taken by the first process is Beta distributed .\nPDF: \\[f(x; \\alpha, \\beta) = \\frac{x^{\\alpha-1} (1-x)^{\\beta-1}}{B(\\alpha, \\beta)}\\]\nwhere\n\\[B(\\alpha, \\beta) = \\int_0^1 x^{\\alpha-1} (1-x)^{\\beta-1} dx\\]\nSupport : [0, 1]\n****PDF of beta distribution**** :\n\nx = torch.linspace(0,1,1000)\ndef pdf_fn(x):\n    alpha = 2\n    beta = 7\n    beta_dist = torch.distributions.Beta(alpha, beta)\n    return beta_dist.log_prob(torch.tensor(x)).exp()\npdf_plot(x,pdf_fn(x))\n\n\n\n\nCDF of beta distribution :\nCDF : \\[F_X(x) = \\frac{B(x; \\alpha, \\beta)}{B(\\alpha, \\beta)}\\]\nwhere \\({B(a, b)}\\) is the beta function and \\(B(x; \\alpha, \\beta)\\) is the incomplete gamma function.\n\nimport scipy.stats as stats\nx = torch.linspace(0,1,10000)\ndef cdf_fn(x):\n    alpha = 2.0  \n    beta = 7.0   \n    return stats.beta(alpha, beta).cdf(x)\ncdf_plot(x, cdf_fn(x))\n\n\n\n\n****Inverse CDF**** :\n\\[x = (1-p) \\cdot \\alpha_1^{-1} \\cdot p \\cdot \\beta_1^{-1}\\]\n****Inverse CDF of Bata distribution****\n\nx=torch.linspace(0,1,10000)\ndef icdf_fn(x):\n    alpha = 2.0\n    beta = 7.0\n    return stats.beta.ppf(x, alpha, beta)\nicdf_plot(x,icdf_fn(x))\n\n\n\n\n\nplt.plot(x, pdf_fn(x),linestyle = 'dashed')\nicdf_samples=icdf_fn(x)\nsns.kdeplot(icdf_samples, color='green',shade='True')\nplt.xlabel(\"x\")\nplt.ylabel(\"Density\")\nplt.title(\"Kernel Density Estimation\")\n\nText(0.5, 1.0, 'Kernel Density Estimation')\n\n\n\n\n\n\n\nGamma distrubution\nLet \\(X\\) be a random variable. Then, \\(X\\) is said to follow a gamma distribution with shape a and rate \\(b\\) \\[X \\sim \\text{Gamma}(a, b)\\]\n****PDF**** : \\[f(x; a, b) = \\frac{b^a}{\\Gamma(a)} x^{a-1} e^{-bx}, \\quad x &gt; 0\\] where \\(a&gt;0\\) and \\(b&gt;0\\)\n****PDF of Gamma distribution****\n\nx = torch.linspace(0,10,1000)\ndef pdf_fn(x):\n    alpha = 10.0\n    beta = 2.0\n    gamma_dist = torch.distributions.Gamma(alpha, beta)\n    return gamma_dist.log_prob(x).exp()\npdf_plot(x,pdf_fn(x))\n\n\n\n\n****CDF of Gamma distribution****\n****CDF****: \\[F_X(x) = \\frac{\\gamma(a, bx)}{\\Gamma(a)}\\] where \\(\\Gamma(a)\\) is the gamma function and \\(\\gamma(a, bx)\\) is the lower incomplete gamma function.\n\nx = torch.linspace(0,20,1000)\ndef cdf_fn(x):\n    alpha = 10.0 \n    beta = 2.0   \n    return stats.gamma(alpha, beta).cdf(x)\ncdf_plot(x, cdf_fn(x))\n\n\n\n\n****Inverse CDF of Gamma distribution****\n****Inverse CDF**** :\n\\[x = (-\\log(1-p)) \\cdot \\text{shape}^{-1} \\cdot \\text{scale}\\]\n\nx=torch.linspace(0,1,1000)\ndef icdf_fn(x):\n    shape = 10.0   \n    scale = 2.0  \n    gamma_dist = stats.gamma(shape, scale=scale)\n    return gamma_dist.ppf(x)\nicdf_plot(x,icdf_fn(x))\n\n\n\n\n\nplt.plot(x, pdf_fn(x),linestyle = 'dashed',color='red')\nicdf_samples=icdf_fn(x)\nsns.kdeplot(icdf_samples, color='green',shade='True')\nplt.xlabel(\"x\")\nplt.ylabel(\"Density\")\nplt.title(\"Kernel Density Estimation\")\n\nText(0.5, 1.0, 'Kernel Density Estimation')\n\n\n\n\n\n\n\nImplementing a pseudo-random number generator (PRNG)\nImplementing a pseudo-random number generator (PRNG) - Generates a sequence of numbers that exhibit properties of randomness\nLinear Congruential Generator (LCG) is a simple PRNG algorithm - The LCG algorithm is defined by the recurrence relation:\n\\(X_{n+1} = (a \\cdot X_n + c) \\mod m\\)\n\n\\(X_{n+1}\\) is the Next pseudo-random number.\n\\(X_n\\) is the current pseudo-random number.\n\\(a\\) is the multiplier , determines the period of the generated number.\n\\(c\\) is the increment, shifts the generated sequence.\n\\(m\\) is the modulus, determines the range of values .\n\nInteger Constant\n\\(m,{0&lt;m}\\) — The modulus\n\\(a,0&lt;a&lt;m\\) — The multiplier\n\\(c,0&lt;=c&lt;m\\) — The increment\n\\(X_{0},0&lt;X_{0}&lt;m\\) — The seed / start value\n\n# Function for Linear Congruential Generator\ndef lcg(seed, n_samples):\n    \"\"\"\n    Generates a sequence of pseudo-random numbers using the Linear Congruential Generator (LCG) algorithm.\n\n    Args:\n        seed (int): The seed value for the LCG algorithm.\n        n_samples (int): The number of random numbers to generate.\n\n    Returns:\n        list: A list of pseudo-random numbers normalized to the range [0, 1].\n    \"\"\"\n    a = 1103515245\n    c = 12345\n    m = 2 ** 31\n    random_numbers = []\n\n    for _ in range(n_samples):\n        seed = (a * seed + c) % m\n        random_number = seed / m  # Normalize to range [0, 1]\n        random_numbers.append(random_number)\n\n    return random_numbers\nrandom_numbers = lcg(seed=42, n_samples =1000)\nplt.hist(random_numbers )\nplt.xlabel('Random Number')\nplt.ylabel('Frequency')\n\nText(0, 0.5, 'Frequency')\n\n\n\n\n\n\n\nUniform distribution to Normal sampling\n\n\nBox Muller Method\n\ndef uniform_to_normal_boxmuller(n_samples):\n    \"\"\"\n    Generates a sequence of pseudo-random numbers from a standard normal distribution using the Box-Muller method.\n\n    Args:\n        n_samples (int): The number of random numbers to generate.\n\n    Returns:\n        torch.Tensor: A tensor of pseudo-random numbers following the standard normal distribution.\n    \"\"\"\n    uniform_distribution = dist.Uniform(0, 1)\n    random_numbers = []\n\n    for _ in range(n_samples // 2):\n        u1 = uniform_distribution.sample()\n        u2 = uniform_distribution.sample()\n\n        z1 = torch.sqrt(-2 * torch.log(u1)) * torch.cos(2 * math.pi * u2)\n        z2 = torch.sqrt(-2 * torch.log(u1)) * torch.sin(2 * math.pi * u2)\n\n        random_numbers.append(z1)\n        random_numbers.append(z2)\n\n    if n_samples % 2 != 0:\n        u = uniform_distribution.sample()\n        z = torch.sqrt(-2 * torch.log(u)) * torch.cos(2 * math.pi * uniform_distribution.sample())\n        random_numbers.append(z)\n\n    return torch.stack(random_numbers)"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Taylor Series",
    "section": "",
    "text": "\\[\\begin{equation}\nf(x) = \\sum_{n=0}^{\\infty}\\frac{f^{(n)}(a)}{n!}(x-a)^{n}\n\\end{equation}\\]\nLet \\(f(x)\\) be a function that is \\(n+1\\) times differentiable on an interval \\(I\\) containing \\(a\\) and let \\(P_n(x)\\) be the \\(n\\)th degree Taylor polynomial for \\(f(x)\\) about \\(a\\). Then, there exists a number \\(c\\) between \\(a\\) and \\(x\\) such that: \\[\\begin{equation}\nf(x)=P_n(x)+R_n(x),\n\\end{equation}\\] where the remainder \\(R_n(x)\\) is given by: \\[\\begin{equation}\nR_n(x)=\\frac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1}.\n\\end{equation}\\]\n\nDefine the function to be approximated\n\n\nCode\nimport torch\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\n# Define the sine function to be approximated\ndef f(x):\n    return torch.sin(x)\n\nx = torch.linspace(-3.14, 3.14, 100)\ny = f(x)\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Sine Function')\nplt.show()\n\n\n\n\n\n\n\nFirst order Taylor approximation for f(x) at x = 0\n\n\nCode\nx = torch.tensor([0.], requires_grad=True)\ny = f(x)\napprox = y + torch.autograd.grad(y, x, create_graph=True)[0] * x\nx_vals = torch.linspace(-np.pi, np.pi, 100)\ny_vals = f(x_vals)\napprox_vals = (approx.detach() + torch.autograd.grad(approx, x, create_graph=True)[0] * x_vals).detach()\nplt.plot(x_vals.numpy(), y_vals.numpy(), label='sin(x)')\nplt.plot(x_vals.numpy(), approx_vals.numpy(), label='approx')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nFind the nth order Taylor approximation for f(x) at x = 0\n\n\nCode\ndef fact(n):\n    return math.factorial(n)\n\ndef nth_deriv(f, a, n):\n    if isinstance(a, (float, int)):\n        a = torch.tensor([a], dtype=torch.float, requires_grad=True)\n    else:\n        a = a.clone().detach().requires_grad_(True)\n    \n    y = f(a)\n    for i in range(n):\n        y = torch.autograd.grad(y, a, create_graph=True)[0]\n    return y\n\n\n\n# nth degree Taylor polynomial of f(x) around x=a\ndef taylor(f, x, n):\n    result = torch.zeros_like(x)\n    for i in range(n+1):\n        result += nth_deriv(f, 0, i) / torch.tensor(math.factorial(i), dtype=torch.float32) * (x**i)\n    return result\nx_vals = torch.linspace(-math.pi, math.pi, 200)\nplt.plot(x_vals.numpy(), f(x_vals).numpy(), label='f(x)', lw=5)\nplt.plot(x_vals.numpy(), taylor(f, x_vals, 1).detach().numpy(), label='Taylor approximation, n=1')\nplt.plot(x_vals.numpy(), taylor(f, x_vals, 3).detach().numpy(), label='Taylor approximation, n=3')\nplt.plot(x_vals.numpy(), taylor(f, x_vals, 5).detach().numpy(), label='Taylor approximation, n=5')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nPlot of the function g(x) = x^2 and its Taylor approximations up to degree 1 and degree 2 centered at x=0.\n\n\nCode\nx_vals = torch.linspace(-4, 4, 100)\n\ndef g(x):\n    return x**2\n\ndef taylor(f, x, n):\n    x = x.unsqueeze(-1)\n    y = f(x)\n    for i in range(1, n+1):\n        y += (x - x[0])**i / torch.tensor([math.factorial(i)]).float() * f(x[0] + 0.0)\n    return y\n\n\nplt.plot(x_vals.numpy(), g(x_vals).numpy(), label='g(x)', lw=5)\nplt.plot(x_vals.numpy(), taylor(g, x_vals, 1).detach().numpy(), label='Taylor approximation, n=1')\nplt.plot(x_vals.numpy(), taylor(g, x_vals, 3).detach().numpy(), label='Taylor approximation, n=3')\nplt.plot(x_vals.numpy(), taylor(g, x_vals, 5).detach().numpy(), label='Taylor approximation, n=5')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/basic_matplotlib.html",
    "href": "posts/basic_matplotlib.html",
    "title": "Visualization",
    "section": "",
    "text": "# # Simple Line Plots\n# All plots is the visualization of a single function $y = f(x)$.\n# Here we will take a first look at creating a simple plot of this type.\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n#plt.style.use('seaborn-whitegrid')\nimport numpy as np\nFor all Matplotlib plots, we start by creating a figure and axes.\nfig = plt.figure()\nax = plt.axes()\nThe figure* (an instance of the class plt.Figure) can be thought of as a single container that contains all the objects representing axes, graphics, text, and labels.*\nThe axes* (an instance of the class plt.Axes) is what we see above: a bounding box with ticks, grids, and labels.*\nfig = plt.figure()\nax = plt.axes()\n\nx = np.linspace(0, 10, 1000)\nax.plot(x, np.sin(x));\nNote that the semicolon at the end of the last line is intentional: it suppresses the textual representation of the plot from the output.\nAlternatively, we can use the PyLab interface\nplt.plot(x, np.sin(x));"
  },
  {
    "objectID": "posts/basic_matplotlib.html#adjusting-the-plot-line-colors-and-styles",
    "href": "posts/basic_matplotlib.html#adjusting-the-plot-line-colors-and-styles",
    "title": "Visualization",
    "section": "Adjusting the Plot: Line Colors and Styles",
    "text": "Adjusting the Plot: Line Colors and Styles\n\nplt.plot(x, np.sin(x - 0), color='blue',linestyle='solid') # specify color by name\nplt.plot(x, np.sin(x - 1), color='g',linestyle='dashed') # short color code (rgbcmyk)\nplt.plot(x, np.sin(x - 2), color='0.75',linestyle='dashdot') # grayscale between 0 and 1\nplt.plot(x, np.sin(x - 3), color='#FFDD44',linestyle='dotted')# hex code (RRGGBB, 00 to FF)\nplt.plot(x, np.sin(x - 4), color=(1.0,0.2,0.3),linestyle='--') # RGB tuple, values 0 to 1\nplt.plot(x, np.sin(x - 5), color='chartreuse',linestyle=':'); # HTML color names supported\n\n\n\n\nCombining these linestyle and color codes into a single non-keyword argument\n\nplt.plot(x, x + 0, '-g')   # solid green\nplt.plot(x, x + 1, '--c')  # dashed cyan\nplt.plot(x, x + 2, '-.k')  # dashdot black\nplt.plot(x, x + 3, ':r');  # dotted red\n\n\n\n\nRGB (Red/Green/Blue) and CMYK (Cyan/Magenta/Yellow/blacK) color systems, commonly used for digital color graphics."
  },
  {
    "objectID": "posts/basic_matplotlib.html#adjusting-the-plot-axes-limits",
    "href": "posts/basic_matplotlib.html#adjusting-the-plot-axes-limits",
    "title": "Visualization",
    "section": "Adjusting the Plot: Axes Limits",
    "text": "Adjusting the Plot: Axes Limits\n\nplt.plot(x, np.sin(x))\n\nplt.xlim(-1, 11)\nplt.ylim(-1.5, 1.5);\n\n\n\n\nAutomatically tighten the bounds around the current content, as shown in the following figure:\n\nplt.plot(x, np.sin(x))\nplt.axis('tight');\n\n\n\n\nOr you can specify that you want an equal axis ratio, such that one unit in x is visually equivalent to one unit in y, as seen in the following figure:"
  },
  {
    "objectID": "posts/basic_matplotlib.html#labeling-plots",
    "href": "posts/basic_matplotlib.html#labeling-plots",
    "title": "Visualization",
    "section": "Labeling Plots",
    "text": "Labeling Plots\nLabeling of plots: titles, axis labels, and simple legends.\n\nplt.plot(x, np.sin(x))\nplt.title(\"A Sine Curve\")\nplt.xlabel(\"x\")                                         \nplt.ylabel(\"sin(x)\");\n\n\n\n\n\nplt.plot(x, np.sin(x), '-g', label='sin(x)')\n\nplt.axis('equal')\n\nplt.legend();"
  },
  {
    "objectID": "posts/basic_matplotlib.html#matplotlib-gotchas",
    "href": "posts/basic_matplotlib.html#matplotlib-gotchas",
    "title": "Visualization",
    "section": "Matplotlib Gotchas",
    "text": "Matplotlib Gotchas\nWhile most plt functions translate directly to ax methods (plt.plot → ax.plot, plt.legend → ax.legend, etc.), this is not the case for all commands. In particular, functions to set limits, labels, and titles are slightly modified. For transitioning between MATLAB-style functions and object-oriented methods, make the following changes:\n\nplt.xlabel → ax.set_xlabel\nplt.ylabel → ax.set_ylabel\nplt.xlim → ax.set_xlim\nplt.ylim → ax.set_ylim\nplt.title → ax.set_title"
  },
  {
    "objectID": "posts/basic_matplotlib.html#scatter-plots-with-plt.scatter",
    "href": "posts/basic_matplotlib.html#scatter-plots-with-plt.scatter",
    "title": "Visualization",
    "section": "Scatter Plots with plt.scatter",
    "text": "Scatter Plots with plt.scatter\n\nplt.scatter(x, y, marker='o');\n\n\n\n\nLet’s show this by creating a random scatter plot with points of many colors and sizes.\n\nrng = np.random.default_rng(0)\nx = rng.normal(size=100)\ny = rng.normal(size=100)\ncolors = rng.random(100)\nsizes = 1000 * rng.random(100)\n\nplt.scatter(x, y, c=colors, s=sizes, alpha=0.3)\nplt.colorbar();  # show color scale\n\n\n\n\nIn this way, the color and size of points can be used to convey information in the visualization\n\nfrom sklearn.datasets import load_iris\niris = load_iris()\nfeatures = iris.data.T\n\nplt.scatter(features[0], features[1], alpha=0.4,\n            s=100*features[3], c=iris.target, cmap='viridis')\nplt.xlabel(iris.feature_names[0])\nplt.ylabel(iris.feature_names[1]);"
  },
  {
    "objectID": "posts/basic_matplotlib.html#basic-errorbars",
    "href": "posts/basic_matplotlib.html#basic-errorbars",
    "title": "Visualization",
    "section": "Basic Errorbars",
    "text": "Basic Errorbars\n\nx = np.linspace(0, 10, 50)\ndy = 0.8\ny = np.sin(x) + dy * np.random.randn(50)\n\nplt.errorbar(x, y, yerr=dy, fmt='.k');\n\n\n\n\nThe fmt is a format code controlling the appearance of lines and points\n\nplt.errorbar(x, y, yerr=dy, fmt='o', color='black',\n             ecolor='lightgray', elinewidth=3, capsize=0);"
  },
  {
    "objectID": "posts/basic_matplotlib.html#continuous-errors",
    "href": "posts/basic_matplotlib.html#continuous-errors",
    "title": "Visualization",
    "section": "Continuous Errors",
    "text": "Continuous Errors\nIn some situations it is desirable to show errorbars on continuous quantities.\n\nfrom sklearn.gaussian_process import GaussianProcessRegressor\n\n# define the model and draw some data\nmodel = lambda x: x * np.sin(x)\nxdata = np.array([1, 3, 5, 6, 8])\nydata = model(xdata)\n\n# Compute the Gaussian process fit\ngp = GaussianProcessRegressor()\ngp.fit(xdata[:, np.newaxis], ydata)\n\nxfit = np.linspace(0, 10, 1000)\nyfit, dyfit = gp.predict(xfit[:, np.newaxis], return_std=True)\n\n# Visualize the result\nplt.plot(xdata, ydata, 'or')\nplt.plot(xfit, yfit, '-', color='gray')\nplt.fill_between(xfit, yfit - dyfit, yfit + dyfit,\n                 color='gray', alpha=0.2)\nplt.xlim(0, 10);"
  },
  {
    "objectID": "posts/basic_matplotlib.html#two-dimensional-histograms-and-binnings",
    "href": "posts/basic_matplotlib.html#two-dimensional-histograms-and-binnings",
    "title": "Visualization",
    "section": "Two-Dimensional Histograms and Binnings",
    "text": "Two-Dimensional Histograms and Binnings\n\nmean = [0, 0]\ncov = [[1, 1], [1, 2]]\nx, y = rng.multivariate_normal(mean, cov, 10000).T\n\n\nplt.hist2d(x, y, bins=30)\ncb = plt.colorbar()\ncb.set_label('counts in bin')\n\n\n\n\n\nplt.hexbin: Hexagonal binnings\nThe two-dimensional histogram creates a tesselation of squares across the axes.\n\nplt.hexbin(x, y, gridsize=30)\ncb = plt.colorbar(label='count in bin')\n\n\n\n\n\n\nKernel density estimation\nAnother common method for estimating and representing densities in multiple dimensions is kernel density estimation* (KDE).*\n\nfrom scipy.stats import gaussian_kde\n\n# fit an array of size [Ndim, Nsamples]\ndata = np.vstack([x, y])\nkde = gaussian_kde(data)\n\n# evaluate on a regular grid\nxgrid = np.linspace(-3.5, 3.5, 40)\nygrid = np.linspace(-6, 6, 40)\nXgrid, Ygrid = np.meshgrid(xgrid, ygrid)\nZ = kde.evaluate(np.vstack([Xgrid.ravel(), Ygrid.ravel()]))\n\n# Plot the result as an image\nplt.imshow(Z.reshape(Xgrid.shape),\n           origin='lower', aspect='auto',\n           extent=[-3.5, 3.5, -6, 6])\ncb = plt.colorbar()\ncb.set_label(\"density\")"
  },
  {
    "objectID": "posts/basic_matplotlib.html#legend-for-size-of-points",
    "href": "posts/basic_matplotlib.html#legend-for-size-of-points",
    "title": "Visualization",
    "section": "Legend for Size of Points",
    "text": "Legend for Size of Points\nSometimes the legend defaults are not sufficient for the given visualization. For example, perhaps you’re using the size of points to mark certain features of the data, and want to create a legend reflecting this.\n\n# import pandas as pd\n# cities = pd.read_csv('data/california_cities.csv')\n\n# # Extract the data we're interested in\n# lat, lon = cities['latd'], cities['longd']\n# population, area = cities['population_total'], cities['area_total_km2']\n\n# # Scatter the points, using size and color but no label\n# plt.scatter(lon, lat, label=None,\n#             c=np.log10(population), cmap='viridis',\n#             s=area, linewidth=0, alpha=0.5)\n# plt.axis('equal')\n# plt.xlabel('longitude')\n# plt.ylabel('latitude')\n# plt.colorbar(label='log$_{10}$(population)')\n# plt.clim(3, 7)\n\n# # Here we create a legend:\n# # we'll plot empty lists with the desired size and label\n# for area in [100, 300, 500]:\n#     plt.scatter([], [], c='k', alpha=0.3, s=area,\n#                 label=str(area) + ' km$^2$')\n# plt.legend(scatterpoints=1, frameon=False, labelspacing=1, title='City Area')\n\n# plt.title('California Cities: Area and Population');"
  },
  {
    "objectID": "posts/basic_matplotlib.html#plt.subplot-simple-grids-of-subplots",
    "href": "posts/basic_matplotlib.html#plt.subplot-simple-grids-of-subplots",
    "title": "Visualization",
    "section": "plt.subplot: Simple Grids of Subplots",
    "text": "plt.subplot: Simple Grids of Subplots\n\nfig = plt.figure()\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nfor i in range(1, 7):\n    ax = fig.add_subplot(2, 3, i)\n    ax.text(0.5, 0.5, str((2, 3, i)),\n           fontsize=18, ha='center')"
  },
  {
    "objectID": "posts/basic_matplotlib.html#plt.subplots-the-whole-grid-in-one-go",
    "href": "posts/basic_matplotlib.html#plt.subplots-the-whole-grid-in-one-go",
    "title": "Visualization",
    "section": "plt.subplots: The Whole Grid in One Go",
    "text": "plt.subplots: The Whole Grid in One Go\nLet’s create a \\(2 \\times 3\\) grid of subplots, where all axes in the same row share their y-axis scale, and all axes in the same column share their x-axis scale\n\nfig, ax = plt.subplots(2, 3, sharex='col', sharey='row')"
  },
  {
    "objectID": "posts/basic_matplotlib.html#plt.gridspec-more-complicated-arrangements",
    "href": "posts/basic_matplotlib.html#plt.gridspec-more-complicated-arrangements",
    "title": "Visualization",
    "section": "plt.GridSpec: More Complicated Arrangements",
    "text": "plt.GridSpec: More Complicated Arrangements\nA GridSpec for a grid of two rows and three columns with some specified width and height space.\n\ngrid = plt.GridSpec(2, 3, wspace=0.4, hspace=0.3)\nplt.subplot(grid[0, 0])\nplt.subplot(grid[0, 1:])\nplt.subplot(grid[1, :2])\nplt.subplot(grid[1, 2]);\n\n\n\n\n\n# Create some normally distributed data\nmean = [0, 0]\ncov = [[1, 1], [1, 2]]\nrng = np.random.default_rng(1701)\nx, y = rng.multivariate_normal(mean, cov, 3000).T\n\n# Set up the axes with GridSpec\nfig = plt.figure(figsize=(6, 6))\ngrid = plt.GridSpec(4, 4, hspace=0.2, wspace=0.2)\nmain_ax = fig.add_subplot(grid[:-1, 1:])\ny_hist = fig.add_subplot(grid[:-1, 0], xticklabels=[], sharey=main_ax)\nx_hist = fig.add_subplot(grid[-1, 1:], yticklabels=[], sharex=main_ax)\n\n# Scatter points on the main axes\nmain_ax.plot(x, y, 'ok', markersize=3, alpha=0.2)\n\n# Histogram on the attached axes\nx_hist.hist(x, 40, histtype='stepfilled',\n            orientation='vertical', color='gray')\nx_hist.invert_yaxis()\n\ny_hist.hist(y, 40, histtype='stepfilled',\n            orientation='horizontal', color='gray')\ny_hist.invert_xaxis()"
  },
  {
    "objectID": "posts/log-sum-exp.html",
    "href": "posts/log-sum-exp.html",
    "title": "The log-sum-exp trick",
    "section": "",
    "text": "The log-sum-exp trick\nThe log-sum-exp trick improves stability in exponentiation and logarithmic calculations by operating in the logarithmic domain, avoiding numerical overflow or underflow.\n\nimport numpy as np\n\nThe log1p function is a useful tool for calculating the logarithm of 1 + p, particularly when p is a small value. It helps avoid numerical instability issues that can arise when directly computing np.log(1 + p) when p is close to zero. By using log1p, you can achieve a more accurate and stable calculation of the logarithm.\n\nx = np.array([1, 2, 3, 4, 5])\np = 1e-10\nresult = np.log(1 + p)  # Direct computation, can be unstable for small p\nresult_log1p = np.log1p(p)  # More stable computation using log1p\nprint(result)\nprint(result_log1p)\n\n1.000000082690371e-10\n9.999999999500001e-11\n\n\nWhen dealing with small values like 1e-5, using the expression exp(p) - 1 can result in a loss of precision. In such cases, the function expm1(p) is a better alternative as it provides a more accurate result that preserves the precision of the small input value p.\n\nresult1 = np.exp(p) - 1\nresult2 = np.expm1(p)\n\nprint(\"Using exp(p) - 1:\")\nprint(result1)\n\nprint(\"Using expm1(p):\")\nprint(result2)\n\nUsing exp(p) - 1:\n1.000000082740371e-10\nUsing expm1(p):\n1.00000000005e-10\n\n\nUsing np.log1p to compute log(1 + p) provides a numerically stable and accurate result of 1e-10, even for small values of p.\n\nx = 0\ny = 10\n\nresult = np.multiply(x, np.log(y))\n\nprint(result)\n\n0.0\n\n\nTo avoid numerical instability when computing x log(1 + p) with small values of x and p, it is advisable to use np.log1p instead of directly calculating np.log(1 + p). This ensures better accuracy and stability by preserving the precision of the small input p.\n\nfrom scipy.special import xlog1py\nfrom scipy.special import xlogy\n\n\nx = 1\np = 1e-10\n\nresult = xlogy(x, 1 + p)\nresult1 = xlog1py(x, p)\n\nprint(result)\nprint(result1)\n\n1.000000082690371e-10\n9.999999999500001e-11"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blogs",
    "section": "",
    "text": "Distribution\n\n\n\n\n\n\n\nProbability\n\n\n\n\ndesp\n\n\n\n\n\n\nOct 24, 2022\n\n\nRishabh Mondal\n\n\n\n\n\n\n  \n\n\n\n\nTue Plots\n\n\n\n\n\n\n\nPlots\n\n\n\n\ndesp\n\n\n\n\n\n\nOct 24, 2022\n\n\nRishabh Mondal\n\n\n\n\n\n\n  \n\n\n\n\nTaylor Series\n\n\n\n\n\n\n\nQuarto\n\n\nPython\n\n\n\n\ndesp\n\n\n\n\n\n\nOct 24, 2022\n\n\nRishab Mondal\n\n\n\n\n\n\n  \n\n\n\n\nProbability\n\n\n\n\n\n\n\nProbability\n\n\n\n\ndesp\n\n\n\n\n\n\nOct 24, 2022\n\n\nRishabh Mondal\n\n\n\n\n\n\n  \n\n\n\n\nVisualization\n\n\n\n\n\n\n\nPlots\n\n\n\n\ndesp\n\n\n\n\n\n\nOct 24, 2022\n\n\nRishabh Mondal\n\n\n\n\n\n\n  \n\n\n\n\nLoss and Optimization\n\n\n\n\n\n\n\nOptimization\n\n\n\n\ndesp\n\n\n\n\n\n\nOct 24, 2022\n\n\nRishabh Mondal and Khush Shah\n\n\n\n\n\n\n  \n\n\n\n\nThe log-sum-exp trick\n\n\n\n\n\n\n\ntrick\n\n\n\n\ndesp\n\n\n\n\n\n\nOct 24, 2022\n\n\nRishab Mondal\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rishabh Mondal",
    "section": "",
    "text": "Hello, I’m Rishabh, a dedicated JRF student in Computer Science and Engineering at the Sustainability Lab, IIT Gandhinagar, under the guidance of Prof. Nipun Batra. My research revolves around the captivating realms of Computer Vision, with a specific focus on its application in monitoring and analyzing air pollution source detection using satellite imagery. Passionate about leveraging technology for environmental sustainability, I am committed to making meaningful contributions to the field."
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "Rishabh Mondal",
    "section": "Research Interests",
    "text": "Research Interests\n\nMachine Learning\nComputer Vision\nSelf-Supervised Learning"
  }
]