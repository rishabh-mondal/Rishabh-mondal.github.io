[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rishabh Mondal",
    "section": "",
    "text": "Hi! I am Rishabh Mondal.As an enthusiastic learner, I am always eager to take up challenging tasks and push my limits to achieve excellence. My academic journey has give me a strong foundation in the concepts of computer science, and I have gained practical experience through various projects\n\n\nM.Tech in Information Technology\nIndian Institute of Engineering Science and Technology, Shibpur | Sept 2021 - June 2023\nB.Tech in Computer Science and Engineering\nThe Neotia University, Kolkata |Sept 2017 - June 2021\n\n\n\n\nAn LSTM-based Fall Detection System with ROC optimization technique: A step towards more accuracy\nBrain Tumor Detection using Convolution Neural Network.\nIndian currency detection through KNN and audio transfer for blind people.\n\n\n\n\nWinner of IBM ICE DAY (poster competition)|2019"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Rishabh Mondal",
    "section": "",
    "text": "M.Tech in Information Technology\nIndian Institute of Engineering Science and Technology, Shibpur | Sept 2021 - June 2023\nB.Tech in Computer Science and Engineering\nThe Neotia University, Kolkata |Sept 2017 - June 2021"
  },
  {
    "objectID": "index.html#project",
    "href": "index.html#project",
    "title": "Rishabh Mondal",
    "section": "",
    "text": "An LSTM-based Fall Detection System with ROC optimization technique: A step towards more accuracy\nBrain Tumor Detection using Convolution Neural Network.\nIndian currency detection through KNN and audio transfer for blind people."
  },
  {
    "objectID": "index.html#awards",
    "href": "index.html#awards",
    "title": "Rishabh Mondal",
    "section": "",
    "text": "Winner of IBM ICE DAY (poster competition)|2019"
  },
  {
    "objectID": "taylor.html",
    "href": "taylor.html",
    "title": "Blogs",
    "section": "",
    "text": "\\[\\begin{equation}\nf(x) = \\sum_{n=0}^{\\infty}\\frac{f^{(n)}(a)}{n!}(x-a)^{n}\n\\end{equation}\\]\nLet \\(f(x)\\) be a function that is \\(n+1\\) times differentiable on an interval \\(I\\) containing \\(a\\) and let \\(P_n(x)\\) be the \\(n\\)th degree Taylor polynomial for \\(f(x)\\) about \\(a\\). Then, there exists a number \\(c\\) between \\(a\\) and \\(x\\) such that: \\[\\begin{equation}\nf(x)=P_n(x)+R_n(x),\n\\end{equation}\\] where the remainder \\(R_n(x)\\) is given by: \\[\\begin{equation}\nR_n(x)=\\frac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1}.\n\\end{equation}\\]\n\nDefine the function to be approximated\n\n\nCode\nimport torch\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\n# Define the sine function to be approximated\ndef f(x):\n    return torch.sin(x)\n\nx = torch.linspace(-3.14, 3.14, 100)\ny = f(x)\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Sine Function')\nplt.show()\n\n\n\n\n\n\n\nFirst order Taylor approximation for f(x) at x = 0\n\n\nCode\nx = torch.tensor([0.], requires_grad=True)\ny = f(x)\napprox = y + torch.autograd.grad(y, x, create_graph=True)[0] * x\nx_vals = torch.linspace(-np.pi, np.pi, 100)\ny_vals = f(x_vals)\napprox_vals = (approx.detach() + torch.autograd.grad(approx, x, create_graph=True)[0] * x_vals).detach()\nplt.plot(x_vals.numpy(), y_vals.numpy(), label='sin(x)')\nplt.plot(x_vals.numpy(), approx_vals.numpy(), label='approx')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nFind the nth order Taylor approximation for f(x) at x = 0\n\n\nCode\ndef fact(n):\n    return math.factorial(n)\n\ndef nth_deriv(f, a, n):\n    if isinstance(a, (float, int)):\n        a = torch.tensor([a], dtype=torch.float, requires_grad=True)\n    else:\n        a = a.clone().detach().requires_grad_(True)\n    \n    y = f(a)\n    for i in range(n):\n        y = torch.autograd.grad(y, a, create_graph=True)[0]\n    return y\n\n\n\n# nth degree Taylor polynomial of f(x) around x=a\ndef taylor(f, x, n):\n    result = torch.zeros_like(x)\n    for i in range(n+1):\n        result += nth_deriv(f, 0, i) / torch.tensor(math.factorial(i), dtype=torch.float32) * (x**i)\n    return result\nx_vals = torch.linspace(-math.pi, math.pi, 200)\nplt.plot(x_vals.numpy(), f(x_vals).numpy(), label='f(x)', lw=5)\nplt.plot(x_vals.numpy(), taylor(f, x_vals, 1).detach().numpy(), label='Taylor approximation, n=1')\nplt.plot(x_vals.numpy(), taylor(f, x_vals, 3).detach().numpy(), label='Taylor approximation, n=3')\nplt.plot(x_vals.numpy(), taylor(f, x_vals, 5).detach().numpy(), label='Taylor approximation, n=5')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nPlot of the function g(x) = x^2 and its Taylor approximations up to degree 1 and degree 2 centered at x=0.\n\n\nCode\nx_vals = torch.linspace(-4, 4, 100)\n\ndef g(x):\n    return x**2\n\ndef taylor(f, x, n):\n    x = x.unsqueeze(-1)\n    y = f(x)\n    for i in range(1, n+1):\n        y += (x - x[0])**i / torch.tensor([math.factorial(i)]).float() * f(x[0] + 0.0)\n    return y\n\n\nplt.plot(x_vals.numpy(), g(x_vals).numpy(), label='g(x)', lw=5)\nplt.plot(x_vals.numpy(), taylor(g, x_vals, 1).detach().numpy(), label='Taylor approximation, n=1')\nplt.plot(x_vals.numpy(), taylor(g, x_vals, 3).detach().numpy(), label='Taylor approximation, n=3')\nplt.plot(x_vals.numpy(), taylor(g, x_vals, 5).detach().numpy(), label='Taylor approximation, n=5')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "workshop.html",
    "href": "workshop.html",
    "title": "Workshop",
    "section": "",
    "text": "Workshop on Cloud Computing organized by IIT Kharagpur,2019"
  },
  {
    "objectID": "Conference.html",
    "href": "Conference.html",
    "title": "Conference",
    "section": "",
    "text": "Workshop on Cloud Computing organized by IIT Kharagpur,2019"
  }
]