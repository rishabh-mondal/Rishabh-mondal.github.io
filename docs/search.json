[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rishabh Mondal",
    "section": "",
    "text": "Hi! I am Rishabh Mondal.As an enthusiastic learner, I am always eager to take up challenging tasks and push my limits to achieve excellence. My academic journey has give me a strong foundation in the concepts of computer science, and I have gained practical experience through various projects\n\n\nM.Tech in Information Technology\nIndian Institute of Engineering Science and Technology, Shibpur | Sept 2021 - June 2023\nB.Tech in Computer Science and Engineering\nThe Neotia University, Kolkata |Sept 2017 - June 2021\n\n\n\n\nAn LSTM-based Fall Detection System with ROC optimization technique: A step towards more accuracy\nBrain Tumor Detection using Convolution Neural Network.\nIndian currency detection through KNN and audio transfer for blind people.\n\n\n\n\nWinner of IBM ICE DAY (poster competition)|2019"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Rishabh Mondal",
    "section": "",
    "text": "M.Tech in Information Technology\nIndian Institute of Engineering Science and Technology, Shibpur | Sept 2021 - June 2023\nB.Tech in Computer Science and Engineering\nThe Neotia University, Kolkata |Sept 2017 - June 2021"
  },
  {
    "objectID": "index.html#project",
    "href": "index.html#project",
    "title": "Rishabh Mondal",
    "section": "",
    "text": "An LSTM-based Fall Detection System with ROC optimization technique: A step towards more accuracy\nBrain Tumor Detection using Convolution Neural Network.\nIndian currency detection through KNN and audio transfer for blind people."
  },
  {
    "objectID": "index.html#awards",
    "href": "index.html#awards",
    "title": "Rishabh Mondal",
    "section": "",
    "text": "Winner of IBM ICE DAY (poster competition)|2019"
  },
  {
    "objectID": "taylor.html",
    "href": "taylor.html",
    "title": "Taylor Series",
    "section": "",
    "text": "\\[\\begin{equation}\nf(x) = \\sum_{n=0}^{\\infty}\\frac{f^{(n)}(a)}{n!}(x-a)^{n}\n\\end{equation}\\]\nLet \\(f(x)\\) be a function that is \\(n+1\\) times differentiable on an interval \\(I\\) containing \\(a\\) and let \\(P_n(x)\\) be the \\(n\\)th degree Taylor polynomial for \\(f(x)\\) about \\(a\\). Then, there exists a number \\(c\\) between \\(a\\) and \\(x\\) such that: \\[\\begin{equation}\nf(x)=P_n(x)+R_n(x),\n\\end{equation}\\] where the remainder \\(R_n(x)\\) is given by: \\[\\begin{equation}\nR_n(x)=\\frac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1}.\n\\end{equation}\\]\n\nDefine the function to be approximated\n\n\nCode\nimport torch\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\n# Define the sine function to be approximated\ndef f(x):\n    return torch.sin(x)\n\nx = torch.linspace(-3.14, 3.14, 100)\ny = f(x)\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Sine Function')\nplt.show()\n\n\n\n\n\n\n\nFirst order Taylor approximation for f(x) at x = 0\n\n\nCode\nx = torch.tensor([0.], requires_grad=True)\ny = f(x)\napprox = y + torch.autograd.grad(y, x, create_graph=True)[0] * x\nx_vals = torch.linspace(-np.pi, np.pi, 100)\ny_vals = f(x_vals)\napprox_vals = (approx.detach() + torch.autograd.grad(approx, x, create_graph=True)[0] * x_vals).detach()\nplt.plot(x_vals.numpy(), y_vals.numpy(), label='sin(x)')\nplt.plot(x_vals.numpy(), approx_vals.numpy(), label='approx')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nFind the nth order Taylor approximation for f(x) at x = 0\n\n\nCode\ndef fact(n):\n    return math.factorial(n)\n\ndef nth_deriv(f, a, n):\n    if isinstance(a, (float, int)):\n        a = torch.tensor([a], dtype=torch.float, requires_grad=True)\n    else:\n        a = a.clone().detach().requires_grad_(True)\n    \n    y = f(a)\n    for i in range(n):\n        y = torch.autograd.grad(y, a, create_graph=True)[0]\n    return y\n\n\n\n# nth degree Taylor polynomial of f(x) around x=a\ndef taylor(f, x, n):\n    result = torch.zeros_like(x)\n    for i in range(n+1):\n        result += nth_deriv(f, 0, i) / torch.tensor(math.factorial(i), dtype=torch.float32) * (x**i)\n    return result\nx_vals = torch.linspace(-math.pi, math.pi, 200)\nplt.plot(x_vals.numpy(), f(x_vals).numpy(), label='f(x)', lw=5)\nplt.plot(x_vals.numpy(), taylor(f, x_vals, 1).detach().numpy(), label='Taylor approximation, n=1')\nplt.plot(x_vals.numpy(), taylor(f, x_vals, 3).detach().numpy(), label='Taylor approximation, n=3')\nplt.plot(x_vals.numpy(), taylor(f, x_vals, 5).detach().numpy(), label='Taylor approximation, n=5')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nPlot of the function g(x) = x^2 and its Taylor approximations up to degree 1 and degree 2 centered at x=0.\n\n\nCode\nx_vals = torch.linspace(-4, 4, 100)\n\ndef g(x):\n    return x**2\n\ndef taylor(f, x, n):\n    x = x.unsqueeze(-1)\n    y = f(x)\n    for i in range(1, n+1):\n        y += (x - x[0])**i / torch.tensor([math.factorial(i)]).float() * f(x[0] + 0.0)\n    return y\n\n\nplt.plot(x_vals.numpy(), g(x_vals).numpy(), label='g(x)', lw=5)\nplt.plot(x_vals.numpy(), taylor(g, x_vals, 1).detach().numpy(), label='Taylor approximation, n=1')\nplt.plot(x_vals.numpy(), taylor(g, x_vals, 3).detach().numpy(), label='Taylor approximation, n=3')\nplt.plot(x_vals.numpy(), taylor(g, x_vals, 5).detach().numpy(), label='Taylor approximation, n=5')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "workshop.html",
    "href": "workshop.html",
    "title": "Workshop",
    "section": "",
    "text": "Workshop on Cloud Computing organized by IIT Kharagpur,2019"
  },
  {
    "objectID": "Conference.html",
    "href": "Conference.html",
    "title": "Conference",
    "section": "",
    "text": "Workshop on Cloud Computing organized by IIT Kharagpur,2019"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blogs",
    "section": "",
    "text": "Probablity Distrubutions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBernoulli distribution\n\n\n\n\n\n\nRishabh Mondal\n\n\nJun 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTaylor Series\n\n\n\nQuarto\n\n\nPython\n\n\n\ndesp\n\n\n\nRishab Mondal\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Taylor Series",
    "section": "",
    "text": "\\[\\begin{equation}\nf(x) = \\sum_{n=0}^{\\infty}\\frac{f^{(n)}(a)}{n!}(x-a)^{n}\n\\end{equation}\\]\nLet \\(f(x)\\) be a function that is \\(n+1\\) times differentiable on an interval \\(I\\) containing \\(a\\) and let \\(P_n(x)\\) be the \\(n\\)th degree Taylor polynomial for \\(f(x)\\) about \\(a\\). Then, there exists a number \\(c\\) between \\(a\\) and \\(x\\) such that: \\[\\begin{equation}\nf(x)=P_n(x)+R_n(x),\n\\end{equation}\\] where the remainder \\(R_n(x)\\) is given by: \\[\\begin{equation}\nR_n(x)=\\frac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1}.\n\\end{equation}\\]\n\nDefine the function to be approximated\n\n\nCode\nimport torch\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\n# Define the sine function to be approximated\ndef f(x):\n    return torch.sin(x)\n\nx = torch.linspace(-3.14, 3.14, 100)\ny = f(x)\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Sine Function')\nplt.show()\n\n\n\n\n\n\n\nFirst order Taylor approximation for f(x) at x = 0\n\n\nCode\nx = torch.tensor([0.], requires_grad=True)\ny = f(x)\napprox = y + torch.autograd.grad(y, x, create_graph=True)[0] * x\nx_vals = torch.linspace(-np.pi, np.pi, 100)\ny_vals = f(x_vals)\napprox_vals = (approx.detach() + torch.autograd.grad(approx, x, create_graph=True)[0] * x_vals).detach()\nplt.plot(x_vals.numpy(), y_vals.numpy(), label='sin(x)')\nplt.plot(x_vals.numpy(), approx_vals.numpy(), label='approx')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nFind the nth order Taylor approximation for f(x) at x = 0\n\n\nCode\ndef fact(n):\n    return math.factorial(n)\n\ndef nth_deriv(f, a, n):\n    if isinstance(a, (float, int)):\n        a = torch.tensor([a], dtype=torch.float, requires_grad=True)\n    else:\n        a = a.clone().detach().requires_grad_(True)\n    \n    y = f(a)\n    for i in range(n):\n        y = torch.autograd.grad(y, a, create_graph=True)[0]\n    return y\n\n\n\n# nth degree Taylor polynomial of f(x) around x=a\ndef taylor(f, x, n):\n    result = torch.zeros_like(x)\n    for i in range(n+1):\n        result += nth_deriv(f, 0, i) / torch.tensor(math.factorial(i), dtype=torch.float32) * (x**i)\n    return result\nx_vals = torch.linspace(-math.pi, math.pi, 200)\nplt.plot(x_vals.numpy(), f(x_vals).numpy(), label='f(x)', lw=5)\nplt.plot(x_vals.numpy(), taylor(f, x_vals, 1).detach().numpy(), label='Taylor approximation, n=1')\nplt.plot(x_vals.numpy(), taylor(f, x_vals, 3).detach().numpy(), label='Taylor approximation, n=3')\nplt.plot(x_vals.numpy(), taylor(f, x_vals, 5).detach().numpy(), label='Taylor approximation, n=5')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nPlot of the function g(x) = x^2 and its Taylor approximations up to degree 1 and degree 2 centered at x=0.\n\n\nCode\nx_vals = torch.linspace(-4, 4, 100)\n\ndef g(x):\n    return x**2\n\ndef taylor(f, x, n):\n    x = x.unsqueeze(-1)\n    y = f(x)\n    for i in range(1, n+1):\n        y += (x - x[0])**i / torch.tensor([math.factorial(i)]).float() * f(x[0] + 0.0)\n    return y\n\n\nplt.plot(x_vals.numpy(), g(x_vals).numpy(), label='g(x)', lw=5)\nplt.plot(x_vals.numpy(), taylor(g, x_vals, 1).detach().numpy(), label='Taylor approximation, n=1')\nplt.plot(x_vals.numpy(), taylor(g, x_vals, 3).detach().numpy(), label='Taylor approximation, n=3')\nplt.plot(x_vals.numpy(), taylor(g, x_vals, 5).detach().numpy(), label='Taylor approximation, n=5')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Certification",
    "section": "",
    "text": "Deep Learning from GUVI Geek Network & IITM - 2021.\nSix weeks of Summer Training on ML from IIT Kanpur,2019.\nOnline Python Training from IIT Kanpur,2019."
  },
  {
    "objectID": "probablity_distribution.html",
    "href": "probablity_distribution.html",
    "title": "Probablity Distrubutions",
    "section": "",
    "text": "Bernoulli Distribution\nTypes of Distribution : Discrete Distribution\nIn Bernoulli Distribution the random variable takes the value \\(1\\) with probability \\(p\\) and the value \\(0\\) with probability \\(1-p\\), where \\(0 ≤ p ≤ 1\\).\nThe probability mass function (PMF): \\[\\begin{equation}\nP(X = x) = p^x \\cdot (1-p)^{1-x} \\tag{1}\n\\end{equation}\\]\nWhere \\((X)\\) is the random variable, \\((x)\\) can be either 0 or \\(1\\), and \\((p)\\) is the probability of success.\n\n\nCode\np=0.6 #success=0.6 failure=0.4\nimport numpy as np\nsample = np.random.choice([0, 1], p=[1 - p, p])\nprob = (p ** sample) * ((1 - p) ** (1 - sample))\nprint(\"Sample:\", sample)\nprint(\"Probability:\", prob)\n\n#Using PyTorch\nimport torch\nfrom torch.distributions import Bernoulli\ndist=Bernoulli(torch.tensor([p]))\nsample=dist.sample()\nprint(\"Sample:\", sample)\nprint(\"Probability:\", dist)\n\n#Set of Probablty of success\nprobs = torch.tensor([0.7, 0.4, 0.9])\nbernoulli_dist = Bernoulli(probs=probs,logits=None)\nsamples = bernoulli_dist.sample()\nprint(\"probablity distributions:\", bernoulli_dist)\nprint(\"Samples:\", samples)\n\n# Log-odds of success\nlogits = torch.tensor([0.847])\ndist = Bernoulli(probs=None,logits=logits)\nsample = dist.sample()\nprint(\"log odd prob :\", dist)\nprint(\"Sample:\", sample.item())\n\n\nSample: 0\nProbability: 0.4\nSample: tensor([0.])\nProbability: Bernoulli(probs: tensor([0.6000]))\nprobablity distributions: Bernoulli(probs: torch.Size([3]))\nSamples: tensor([0., 0., 1.])\nlog odd prob : Bernoulli(probs: tensor([0.6999]), logits: tensor([0.8470]))\nSample: 0.0\n\n\nLog probability of Bernoulli distribution\nTo obtain the log probability, we take the natural logarithm of the PMF: \\[\\begin{equation}\n\\log P(X=x) = \\log(p^x \\cdot (1-p)^{1-x}) \\tag{2}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\log P(X=x) = x \\cdot \\log(p) + (1-x) \\cdot \\log(1-p) \\tag{3}\n\\end{equation}\\]\n\n\nCode\nimport math\nsample=1\nprob=0.6\nlog_probability = sample * math.log(p) + (1 - sample) * math.log(1 - p)\nprint(\"sample:\", sample)\nprint(\"Log Probability:\", log_probability)\n\n#using PyTorch\nsample = torch.tensor([1])\np = torch.tensor([0.6])\ndist = Bernoulli(probs=p,logits=None)\nsample=dist.sample()\nlog_prob=dist.log_prob(sample)\nprint(\"Sample:\", sample)\nprint(\"Log Probability:\", log_prob)\n\n\nsample: 1\nLog Probability: -0.5108256237659907\nSample: tensor([0.])\nLog Probability: tensor([-0.9163])\n\n\nMaximum Likelihood Estimations(MLE) for Bernoulli Distribution\nThe MLE is a method used to estimate the parameters of a probability distribution based on observed data.\nDerivation of MLE for Bernoulli Distribution\nWe have a dataset with n binary samples:\\(x1\\) , \\(x2\\) , ..,\\(xn\\), where each \\(xi\\) is 0 or 1.\nThe likelihood function for the Bernoulli distribution is given by: \\[\\begin{equation}\nL(p) = \\prod_{i=1}^{n} p^{x_i} \\cdot (1-p)^{1-x_i}\n\\end{equation}\\]\nTaking the log-likelihood function: \\[\\begin{equation}\n\\log L(p) = \\sum_{i=1}^{n} x_i \\log p + (1-x_i) \\log (1-p)\n\\end{equation}\\]\nDifferentiating the log-likelihood function and setting it equal to zero: \\[\\begin{equation}\n\\frac{{\\partial}}{{\\partial p}} \\log L(p) = \\sum_{i=1}^{n} \\left(\\frac{{x_i}}{{p}} - \\frac{{1-x_i}}{{1-p}}\\right) = 0\n\\end{equation}\\]\nSimplifying the equation: \\[\\begin{equation}\n\\frac{{\\sum_{i=1}^{n} x_i - np}}{{p(1-p)}} = 0\n\\end{equation}\\]\nSolving for \\(p\\): \\[\\begin{equation}\nnp = \\sum_{i=1}^{n} x_i\n\\end{equation}\\]\n\n\nCode\nsize = 100\ndataset = dist.sample(torch.Size([size]))\nnum_suc=dataset.float().sum()\np_estimate=num_suc.float()/dataset.size(0)\nprint(\"MLE Estimate:\", p_estimate.item())\n\n\nMLE Estimate: 0.550000011920929\n\n\nLoss v/s iteration curve\n\n\nCode\nfrom torch.optim import Adam\nimport matplotlib.pyplot as plt\ndataset_sizes = [10, 50, 100, 200, 500,1000,10000]\ndef negative_log_likelihood(p, dataset):\n    return -(dataset * torch.log(p) + (1 - dataset) * torch.log(1 - p)).mean()\n\nfor size in dataset_sizes:\n   \n    dataset = torch.randint(low=0, high=2, size=(size,))\n    p = torch.tensor(0.5, requires_grad=True)\n    optimizer = Adam([p], lr=0.1)\n    loss_values = []\n    iteration_values = []\n    for i in range(100):\n        optimizer.zero_grad()\n        loss = negative_log_likelihood(p, dataset)\n        loss.backward()\n        optimizer.step()\n        loss_values.append(loss.item())\n        iteration_values.append(i+1)\n    plt.plot(iteration_values, loss_values, label=f'Dataset Size: {size}')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Loss vs. Iteration for Varying Dataset Sizes')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/probablity_distribution.html",
    "href": "posts/probablity_distribution.html",
    "title": "Probablity Distrubutions",
    "section": "",
    "text": "Bernoulli Distribution\nTypes of Distribution : Discrete Distribution\nIn Bernoulli Distribution the random variable takes the value \\(1\\) with probability \\(p\\) and the value \\(0\\) with probability \\(1-p\\), where \\(0 ≤ p ≤ 1\\).\nThe probability mass function (PMF): \\[\\begin{equation}\nP(X = x) = p^x \\cdot (1-p)^{1-x} \\tag{1}\n\\end{equation}\\]\nWhere \\((X)\\) is the random variable, \\((x)\\) can be either 0 or \\(1\\), and \\((p)\\) is the probability of success.\n\n\nCode\np=0.6 #success=0.6 failure=0.4\nimport numpy as np\nsample = np.random.choice([0, 1], p=[1 - p, p])\nprob = (p ** sample) * ((1 - p) ** (1 - sample))\nprint(\"Sample:\", sample)\nprint(\"Probability:\", prob)\n\n#Using PyTorch\nimport torch\nfrom torch.distributions import Bernoulli\ndist=Bernoulli(torch.tensor([p]))\nsample=dist.sample()\nprint(\"Sample:\", sample)\nprint(\"Probability:\", dist)\n\n#Set of Probablty of success\nprobs = torch.tensor([0.7, 0.4, 0.9])\nbernoulli_dist = Bernoulli(probs=probs,logits=None)\nsamples = bernoulli_dist.sample()\nprint(\"probablity distributions:\", bernoulli_dist)\nprint(\"Samples:\", samples)\n\n# Log-odds of success\nlogits = torch.tensor([0.847])\ndist = Bernoulli(probs=None,logits=logits)\nsample = dist.sample()\nprint(\"log odd prob :\", dist)\nprint(\"Sample:\", sample.item())\n\n\nSample: 0\nProbability: 0.4\nSample: tensor([1.])\nProbability: Bernoulli(probs: tensor([0.6000]))\nprobablity distributions: Bernoulli(probs: torch.Size([3]))\nSamples: tensor([0., 0., 1.])\nlog odd prob : Bernoulli(probs: tensor([0.6999]), logits: tensor([0.8470]))\nSample: 0.0\n\n\nLog probability of Bernoulli distribution\nTo obtain the log probability, we take the natural logarithm of the PMF: \\[\\begin{equation}\n\\log P(X=x) = \\log(p^x \\cdot (1-p)^{1-x}) \\tag{2}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\log P(X=x) = x \\cdot \\log(p) + (1-x) \\cdot \\log(1-p) \\tag{3}\n\\end{equation}\\]\n\n\nCode\nimport math\nsample=1\nprob=0.6\nlog_probability = sample * math.log(p) + (1 - sample) * math.log(1 - p)\nprint(\"sample:\", sample)\nprint(\"Log Probability:\", log_probability)\n\n#using PyTorch\nsample = torch.tensor([1])\np = torch.tensor([0.6])\ndist = Bernoulli(probs=p,logits=None)\nsample=dist.sample()\nlog_prob=dist.log_prob(sample)\nprint(\"Sample:\", sample)\nprint(\"Log Probability:\", log_prob)\n\n\nsample: 1\nLog Probability: -0.5108256237659907\nSample: tensor([0.])\nLog Probability: tensor([-0.9163])\n\n\nMaximum Likelihood Estimations(MLE) for Bernoulli Distribution\nThe MLE is a method used to estimate the parameters of a probability distribution based on observed data.\nDerivation of MLE for Bernoulli Distribution\nWe have a dataset with n binary samples:\\(x1\\) , \\(x2\\) , ..,\\(xn\\), where each \\(xi\\) is 0 or 1.\nThe likelihood function for the Bernoulli distribution is given by: \\[\\begin{equation}\nL(p) = \\prod_{i=1}^{n} p^{x_i} \\cdot (1-p)^{1-x_i}\n\\end{equation}\\]\nTaking the log-likelihood function: \\[\\begin{equation}\n\\log L(p) = \\sum_{i=1}^{n} x_i \\log p + (1-x_i) \\log (1-p)\n\\end{equation}\\]\nDifferentiating the log-likelihood function and setting it equal to zero: \\[\\begin{equation}\n\\frac{{\\partial}}{{\\partial p}} \\log L(p) = \\sum_{i=1}^{n} \\left(\\frac{{x_i}}{{p}} - \\frac{{1-x_i}}{{1-p}}\\right) = 0\n\\end{equation}\\]\nSimplifying the equation: \\[\\begin{equation}\n\\frac{{\\sum_{i=1}^{n} x_i - np}}{{p(1-p)}} = 0\n\\end{equation}\\]\nSolving for \\(p\\): \\[\\begin{equation}\nnp = \\sum_{i=1}^{n} x_i\n\\end{equation}\\]\n\n\nCode\nsize = 100\ndataset = dist.sample(torch.Size([size]))\nnum_suc=dataset.float().sum()\np_estimate=num_suc.float()/dataset.size(0)\nprint(\"MLE Estimate:\", p_estimate.item())\n\n\nMLE Estimate: 0.5899999737739563\n\n\nLoss v/s iteration curve\n\n\nCode\nfrom torch.optim import Adam\nimport matplotlib.pyplot as plt\ndataset_sizes = [10, 50, 100, 200, 500,1000,10000]\ndef negative_log_likelihood(p, dataset):\n    return -(dataset * torch.log(p) + (1 - dataset) * torch.log(1 - p)).mean()\n\nfor size in dataset_sizes:\n   \n    dataset = torch.randint(low=0, high=2, size=(size,))\n    p = torch.tensor(0.5, requires_grad=True)\n    optimizer = Adam([p], lr=0.1)\n    loss_values = []\n    iteration_values = []\n    for i in range(100):\n        optimizer.zero_grad()\n        loss = negative_log_likelihood(p, dataset)\n        loss.backward()\n        optimizer.step()\n        loss_values.append(loss.item())\n        iteration_values.append(i+1)\n    plt.plot(iteration_values, loss_values, label=f'Dataset Size: {size}')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Loss vs. Iteration for Varying Dataset Sizes')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/Probablity_distibution.html",
    "href": "posts/Probablity_distibution.html",
    "title": "Bernoulli distribution",
    "section": "",
    "text": "Bernoulli distribution is a discret univariate probability distribution. A Bernoulli trial or experiment results in binary outcomes: success or failure \\((0 or 1)\\). The trial’s success is denoted as $ p (x=1)$, and failure is expressed as \\(1-p ( x=0)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$ \\[\\begin{equation}\nP(X = x) = p^x \\cdot (1-p)^{1-x} \\tag{1}\n\\end{equation}\\] $ where \\((X)\\) is the random variable, \\((x)\\) can be either 0 or 1, and \\((p)\\) is the probability of success.\nSome imports\n\nimport numpy as np\nimport torch\nfrom torch.distributions import Bernoulli\nimport math\nfrom torch.optim import Adam\nimport matplotlib.pyplot as plt\n\nImplementation of PMF\n\np=0.6 #success=0.6 failure=0.4\nimport numpy as np\nsample = np.random.choice([0, 1], p=[1 - p, p])\nprob = (p ** sample) * ((1 - p) ** (1 - sample))\nprint(\"Sample:\", sample)\nprint(\"Probability:\", prob)\n\n#Using PyTorch\nimport torch\nfrom torch.distributions import Bernoulli\ndist=Bernoulli(torch.tensor([p]))\nsample=dist.sample()\nprint(\"Sample:\", sample)\nprint(\"Probability:\", dist)\n\n#Set of Probablty of success\nprobs = torch.tensor([0.7, 0.4, 0.9])\nbernoulli_dist = Bernoulli(probs=probs,logits=None)\nsamples = bernoulli_dist.sample()\nprint(\"probablity distributions:\", bernoulli_dist)\nprint(\"Samples:\", samples)\n\n# Log-odds of success\nlogits = torch.tensor([0.847])\ndist = Bernoulli(probs=None,logits=logits)\nsample = dist.sample()\nprint(\"log odd prob :\", dist)\nprint(\"Sample:\", sample.item())\n\nSample: 1\nProbability: 0.6\nSample: tensor([0.])\nProbability: Bernoulli(probs: tensor([0.6000]))\nprobablity distributions: Bernoulli(probs: torch.Size([3]))\nSamples: tensor([0., 1., 0.])\nlog odd prob : Bernoulli(probs: tensor([0.6999]), logits: tensor([0.8470]))\nSample: 1.0\n\n\nLog probability of Bernoulli distribution\nTo obtain the log probability, we take the natural logarithm of the PMF: $ \\[\\begin{equation}\n\\log P(X=x) = \\log(p^x \\cdot (1-p)^{1-x})\n\\end{equation}\\] $ $ \\[\\begin{equation}\n\\log P(X=x) = x \\cdot \\log(p) + (1-x) \\cdot \\log(1-p)\n\\end{equation}\\] $\n\nsample=1\nprob=0.6\nlog_probability = sample * math.log(p) + (1 - sample) * math.log(1 - p)\nprint(\"sample:\", sample)\nprint(\"Log Probability:\", log_probability)\n\n#using PyTorch\nsample = torch.tensor([1])\np = torch.tensor([0.6])\ndist = Bernoulli(probs=p,logits=None)\nsample=dist.sample()\nlog_prob=dist.log_prob(sample)\nprint(\"Sample:\", sample)\nprint(\"Log Probability:\", log_prob)\n\nsample: 1\nLog Probability: -0.5108256237659907\nSample: tensor([1.])\nLog Probability: tensor([-0.5108])\n\n\nMaximum Likelihood Estimations(MLE) for Bernoulli Distribution\nTo derive the Maximum Likelihood Estimation (MLE) for the Bernoulli distribution, let’s assume we have a random sample of independent and identically distributed (i.i.d.) observations from a Bernoulli distribution with parameter p. Each observation can take a value of either 0 or 1.\nThe likelihood function for the Bernoulli distribution is given by:\n$ L(p) = _{i=1}^{n} p^{x_i} (1-p)^{1-x_i} $\nwhere (x_i) is the i-th observation in the sample and n is the total number of observations.\nTo find the MLE for p, we want to find the value of p that maximizes the likelihood function L(p). It is often easier to work with the log-likelihood function, which is the natural logarithm of the likelihood function:\n$ L(p) = _{i=1}^{n} x_i (p) + (1-x_i) (1-p) $\nTo find the MLE, we differentiate the log-likelihood function with respect to p and set it equal to zero:\n$ (L(p)) = {i=1}^{n} x_i - {i=1}^{n} (1-x_i) = 0 $\nSimplifying the equation:\n$ {i=1}^{n} x_i - + {i=1}^{n} x_i = 0 $\nMultiplying through by p(1-p):\n$ (1-p){i=1}^{n} x_i - np + p{i=1}^{n} x_i = 0 $\nRearranging the terms:\n$ _{i=1}^{n} x_i - np = 0 $ Finally, solving for p:\n$ p = _{i=1}^{n} x_i $\nTherefore, the MLE for the parameter p in the Bernoulli distribution is the sample mean of the observed values.\nIt is important to note that this MLE is consistent, unbiased, and efficient for estimating the parameter p in the Bernoulli distribution.\n\nsize = 100\ndataset = dist.sample(torch.Size([size]))\nnum_suc=dataset.float().sum()\np_estimate=num_suc.float()/dataset.size(0)\nprint(\"MLE Estimate:\", p_estimate.item())\n\nMLE Estimate: 0.6100000143051147\n\n\nPerforming Maximum Likelihood Estimation (MLE) for the Bernoulli distribution with varying dataset sizes. It computes the negative log-likelihood loss for different dataset sizes and optimizes the parameter ‘p’ to minimize the loss using the Adam optimizer.\nThe resulting loss values are then plotted against the iterations to visualize the convergence of the MLE estimation.\n\ndataset_sizes = [10, 50, 100, 200, 500,1000,10000]\ndef negative_log_likelihood(p, dataset):\n    return -(dataset * torch.log(p) + (1 - dataset) * torch.log(1 - p)).mean()\n\nfor size in dataset_sizes:\n   \n    dataset = torch.randint(low=0, high=2, size=(size,))\n    p = torch.tensor(0.5, requires_grad=True)\n    optimizer = Adam([p], lr=0.1)\n    loss_values = []\n    iteration_values = []\n    for i in range(100):\n        optimizer.zero_grad()\n        loss = negative_log_likelihood(p, dataset)\n        loss.backward()\n        optimizer.step()\n        loss_values.append(loss.item())\n        iteration_values.append(i+1)\n    plt.plot(iteration_values, loss_values, label=f'Dataset Size: {size}')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Loss vs. Iteration for Varying Dataset Sizes')\nplt.legend()\nplt.show()\n\n\n\n\nThe plot shows the relationship between the loss and the number of iterations for each dataset size.\nBy examining the plot, we can observe the following:\n\nAs the dataset size increases, the convergence to the optimal parameter value tends to be faster. This is because larger datasets provide more information, allowing for more accurate estimation.\nFor smaller dataset sizes (e.g., 10, 50, 100), the loss tends to fluctuate more initially. However, as the number of iterations increases, the loss converges to a stable value.\nFor larger dataset sizes (e.g., 1000, 10000), the loss tends to converge quickly and stabilize earlier compared to smaller dataset sizes."
  },
  {
    "objectID": "posts/Probablity_distibution.html#introduction",
    "href": "posts/Probablity_distibution.html#introduction",
    "title": "Bernoulli distribution",
    "section": "",
    "text": "Bernoulli distribution is a discret univariate probability distribution. A Bernoulli trial or experiment results in binary outcomes: success or failure \\((0 or 1)\\). The trial’s success is denoted as $ p (x=1)$, and failure is expressed as \\(1-p ( x=0)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$ \\[\\begin{equation}\nP(X = x) = p^x \\cdot (1-p)^{1-x} \\tag{1}\n\\end{equation}\\] $ where \\((X)\\) is the random variable, \\((x)\\) can be either 0 or 1, and \\((p)\\) is the probability of success.\nSome imports\n\nimport numpy as np\nimport torch\nfrom torch.distributions import Bernoulli\nimport math\nfrom torch.optim import Adam\nimport matplotlib.pyplot as plt\n\nImplementation of PMF\n\np=0.6 #success=0.6 failure=0.4\nimport numpy as np\nsample = np.random.choice([0, 1], p=[1 - p, p])\nprob = (p ** sample) * ((1 - p) ** (1 - sample))\nprint(\"Sample:\", sample)\nprint(\"Probability:\", prob)\n\n#Using PyTorch\nimport torch\nfrom torch.distributions import Bernoulli\ndist=Bernoulli(torch.tensor([p]))\nsample=dist.sample()\nprint(\"Sample:\", sample)\nprint(\"Probability:\", dist)\n\n#Set of Probablty of success\nprobs = torch.tensor([0.7, 0.4, 0.9])\nbernoulli_dist = Bernoulli(probs=probs,logits=None)\nsamples = bernoulli_dist.sample()\nprint(\"probablity distributions:\", bernoulli_dist)\nprint(\"Samples:\", samples)\n\n# Log-odds of success\nlogits = torch.tensor([0.847])\ndist = Bernoulli(probs=None,logits=logits)\nsample = dist.sample()\nprint(\"log odd prob :\", dist)\nprint(\"Sample:\", sample.item())\n\nSample: 1\nProbability: 0.6\nSample: tensor([0.])\nProbability: Bernoulli(probs: tensor([0.6000]))\nprobablity distributions: Bernoulli(probs: torch.Size([3]))\nSamples: tensor([0., 1., 0.])\nlog odd prob : Bernoulli(probs: tensor([0.6999]), logits: tensor([0.8470]))\nSample: 1.0\n\n\nLog probability of Bernoulli distribution\nTo obtain the log probability, we take the natural logarithm of the PMF: $ \\[\\begin{equation}\n\\log P(X=x) = \\log(p^x \\cdot (1-p)^{1-x})\n\\end{equation}\\] $ $ \\[\\begin{equation}\n\\log P(X=x) = x \\cdot \\log(p) + (1-x) \\cdot \\log(1-p)\n\\end{equation}\\] $\n\nsample=1\nprob=0.6\nlog_probability = sample * math.log(p) + (1 - sample) * math.log(1 - p)\nprint(\"sample:\", sample)\nprint(\"Log Probability:\", log_probability)\n\n#using PyTorch\nsample = torch.tensor([1])\np = torch.tensor([0.6])\ndist = Bernoulli(probs=p,logits=None)\nsample=dist.sample()\nlog_prob=dist.log_prob(sample)\nprint(\"Sample:\", sample)\nprint(\"Log Probability:\", log_prob)\n\nsample: 1\nLog Probability: -0.5108256237659907\nSample: tensor([1.])\nLog Probability: tensor([-0.5108])\n\n\nMaximum Likelihood Estimations(MLE) for Bernoulli Distribution\nTo derive the Maximum Likelihood Estimation (MLE) for the Bernoulli distribution, let’s assume we have a random sample of independent and identically distributed (i.i.d.) observations from a Bernoulli distribution with parameter p. Each observation can take a value of either 0 or 1.\nThe likelihood function for the Bernoulli distribution is given by:\n$ L(p) = _{i=1}^{n} p^{x_i} (1-p)^{1-x_i} $\nwhere (x_i) is the i-th observation in the sample and n is the total number of observations.\nTo find the MLE for p, we want to find the value of p that maximizes the likelihood function L(p). It is often easier to work with the log-likelihood function, which is the natural logarithm of the likelihood function:\n$ L(p) = _{i=1}^{n} x_i (p) + (1-x_i) (1-p) $\nTo find the MLE, we differentiate the log-likelihood function with respect to p and set it equal to zero:\n$ (L(p)) = {i=1}^{n} x_i - {i=1}^{n} (1-x_i) = 0 $\nSimplifying the equation:\n$ {i=1}^{n} x_i - + {i=1}^{n} x_i = 0 $\nMultiplying through by p(1-p):\n$ (1-p){i=1}^{n} x_i - np + p{i=1}^{n} x_i = 0 $\nRearranging the terms:\n$ _{i=1}^{n} x_i - np = 0 $ Finally, solving for p:\n$ p = _{i=1}^{n} x_i $\nTherefore, the MLE for the parameter p in the Bernoulli distribution is the sample mean of the observed values.\nIt is important to note that this MLE is consistent, unbiased, and efficient for estimating the parameter p in the Bernoulli distribution.\n\nsize = 100\ndataset = dist.sample(torch.Size([size]))\nnum_suc=dataset.float().sum()\np_estimate=num_suc.float()/dataset.size(0)\nprint(\"MLE Estimate:\", p_estimate.item())\n\nMLE Estimate: 0.6100000143051147\n\n\nPerforming Maximum Likelihood Estimation (MLE) for the Bernoulli distribution with varying dataset sizes. It computes the negative log-likelihood loss for different dataset sizes and optimizes the parameter ‘p’ to minimize the loss using the Adam optimizer.\nThe resulting loss values are then plotted against the iterations to visualize the convergence of the MLE estimation.\n\ndataset_sizes = [10, 50, 100, 200, 500,1000,10000]\ndef negative_log_likelihood(p, dataset):\n    return -(dataset * torch.log(p) + (1 - dataset) * torch.log(1 - p)).mean()\n\nfor size in dataset_sizes:\n   \n    dataset = torch.randint(low=0, high=2, size=(size,))\n    p = torch.tensor(0.5, requires_grad=True)\n    optimizer = Adam([p], lr=0.1)\n    loss_values = []\n    iteration_values = []\n    for i in range(100):\n        optimizer.zero_grad()\n        loss = negative_log_likelihood(p, dataset)\n        loss.backward()\n        optimizer.step()\n        loss_values.append(loss.item())\n        iteration_values.append(i+1)\n    plt.plot(iteration_values, loss_values, label=f'Dataset Size: {size}')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Loss vs. Iteration for Varying Dataset Sizes')\nplt.legend()\nplt.show()\n\n\n\n\nThe plot shows the relationship between the loss and the number of iterations for each dataset size.\nBy examining the plot, we can observe the following:\n\nAs the dataset size increases, the convergence to the optimal parameter value tends to be faster. This is because larger datasets provide more information, allowing for more accurate estimation.\nFor smaller dataset sizes (e.g., 10, 50, 100), the loss tends to fluctuate more initially. However, as the number of iterations increases, the loss converges to a stable value.\nFor larger dataset sizes (e.g., 1000, 10000), the loss tends to converge quickly and stabilize earlier compared to smaller dataset sizes."
  }
]