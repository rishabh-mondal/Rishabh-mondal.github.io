
---
title: "Probablity Distrubutions"
format:
  html:
    code-fold: true
jupyter: python3
---

**Bernoulli Distribution**

Types of Distribution : Discrete Distribution


In Bernoulli Distribution the random variable takes the value $1$ with probability $p$ and the value $0$ with probability $1-p$, where $0 ≤ p ≤ 1$.


The probability mass function (PMF):
\begin{equation}
P(X = x) = p^x \cdot (1-p)^{1-x} \tag{1}
\end{equation}


Where $(X)$ is the random variable, $(x)$ can be either 0 or $1$, and $(p)$ is the probability of success.




```{python}
p=0.6 #success=0.6 failure=0.4
import numpy as np
sample = np.random.choice([0, 1], p=[1 - p, p])
prob = (p ** sample) * ((1 - p) ** (1 - sample))
print("Sample:", sample)
print("Probability:", prob)

#Using PyTorch
import torch
from torch.distributions import Bernoulli
dist=Bernoulli(torch.tensor([p]))
sample=dist.sample()
print("Sample:", sample)
print("Probability:", dist)

#Set of Probablty of success
probs = torch.tensor([0.7, 0.4, 0.9])
bernoulli_dist = Bernoulli(probs=probs,logits=None)
samples = bernoulli_dist.sample()
print("probablity distributions:", bernoulli_dist)
print("Samples:", samples)

# Log-odds of success
logits = torch.tensor([0.847])
dist = Bernoulli(probs=None,logits=logits)
sample = dist.sample()
print("log odd prob :", dist)
print("Sample:", sample.item())
```



**Log probability of Bernoulli distribution**

To obtain the log probability, we take the natural logarithm of the PMF:
\begin{equation}
\log P(X=x) = \log(p^x \cdot (1-p)^{1-x}) \tag{2}
\end{equation}

\begin{equation}
\log P(X=x) = x \cdot \log(p) + (1-x) \cdot \log(1-p) \tag{3}
\end{equation}

```{python}
import math
sample=1
prob=0.6
log_probability = sample * math.log(p) + (1 - sample) * math.log(1 - p)
print("sample:", sample)
print("Log Probability:", log_probability)

#using PyTorch
sample = torch.tensor([1])
p = torch.tensor([0.6])
dist = Bernoulli(probs=p,logits=None)
sample=dist.sample()
log_prob=dist.log_prob(sample)
print("Sample:", sample)
print("Log Probability:", log_prob)

```



**Maximum Likelihood Estimations(MLE) for Bernoulli Distribution**

The MLE is a method used to estimate the parameters of a probability distribution based on observed data.

***Derivation of MLE for Bernoulli Distribution***

We have a dataset with n binary samples:$x1$ , $x2$ , ..,$xn$, where each $xi$ is 0 or 1.

The likelihood function for the Bernoulli distribution is given by:
\begin{equation}
L(p) = \prod_{i=1}^{n} p^{x_i} \cdot (1-p)^{1-x_i}
\end{equation}

Taking the log-likelihood function:
\begin{equation}
\log L(p) = \sum_{i=1}^{n} x_i \log p + (1-x_i) \log (1-p)
\end{equation}

Differentiating the log-likelihood function and setting it equal to zero:
\begin{equation}
\frac{{\partial}}{{\partial p}} \log L(p) = \sum_{i=1}^{n} \left(\frac{{x_i}}{{p}} - \frac{{1-x_i}}{{1-p}}\right) = 0
\end{equation}

Simplifying the equation:
\begin{equation}
\frac{{\sum_{i=1}^{n} x_i - np}}{{p(1-p)}} = 0
\end{equation}

Solving for 
$p$:
\begin{equation}
np = \sum_{i=1}^{n} x_i
\end{equation}

```{python}
size = 100
dataset = dist.sample(torch.Size([size]))
num_suc=dataset.float().sum()
p_estimate=num_suc.float()/dataset.size(0)
print("MLE Estimate:", p_estimate.item())


```

Loss v/s iteration curve

```{python}
from torch.optim import Adam
import matplotlib.pyplot as plt
dataset_sizes = [10, 50, 100, 200, 500,1000,10000]
def negative_log_likelihood(p, dataset):
    return -(dataset * torch.log(p) + (1 - dataset) * torch.log(1 - p)).mean()

for size in dataset_sizes:
   
    dataset = torch.randint(low=0, high=2, size=(size,))
    p = torch.tensor(0.5, requires_grad=True)
    optimizer = Adam([p], lr=0.1)
    loss_values = []
    iteration_values = []
    for i in range(100):
        optimizer.zero_grad()
        loss = negative_log_likelihood(p, dataset)
        loss.backward()
        optimizer.step()
        loss_values.append(loss.item())
        iteration_values.append(i+1)
    plt.plot(iteration_values, loss_values, label=f'Dataset Size: {size}')
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.title('Loss vs. Iteration for Varying Dataset Sizes')
plt.legend()
plt.show()

```